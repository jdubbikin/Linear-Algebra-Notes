

# Lecture Notes

## Neads a title

Consider $$ L:V-V $$ , the dimension $$(v) = n$$ The matrix representation depends on an ordered Basis.

####Ex) 
>   $$ L:\mathbb{R}^2 \rightarrow \mathbb{R}^2  $$ 	Were  $$  L(x) = (x-x_1, 3x_2)^T L(e_1) = \left(\begin{array}{ccc} 1 \\ 0 \end{array} \right) $$ ,  $$ L(e_2) = \left[\begin{array}{ccc}-1\\3\end{array}\right] $$


>   The matrix representation with respect to $${\{e_1, e_2\}}$$ is $${\left[\begin{array}{ccc}1 & -1\\0&3\end{array}\right]}$$ 

>   Suppose we choose $$u_1 = \left(\begin{array}{ccc}1\\-1\end{array}\right), u_2 = \left(\begin{array}{ccc}-1\\2\end{array}\right)$$, $$  L(u_ 1) = Au_1 = \left(\begin{array}{ccc} 2 \\ -3 \end{array}\right), L(u_2) = Au_2 = \left(\begin{array}{ccc}-1\\6\end{array}\right)$$

>   We need $$L(u_1) = au_1+cu_2, L(u_2)=bu_1+du_2\implies\left[\begin{array}{ccc}a & b\\c & d\end{array}\right]$$ the transform from $$(u_1, u_2)\left[\begin{array}{ccc}1&1\\-1&2\end{array}\right]$$ the transform from $$ (e_1, e_2)\rightarrow[u_1, u_2]$$  is $$ U^{-1} = \left(\begin{array}{ccc}\frac{2}{3} & \frac{-1}{3}\\ \\\frac{1}{3} & \frac{1}{3}\end{array}\right)$$

>   $$  $$

>   $$ U^{-1} L(u_1) = U^{-1}+Au_1=\left(\begin{array}{ccc}\frac{7}{3}\\ \frac{-1}{3}\end{array}\right)$$ , $$ U^{-1} L(u_2) = U^{-1}+Au_2=\left(\begin{array}{ccc}\frac{8}{3}\\ \frac{5}{3} \end{array} \right)$$

>   $$  $$

>   $$L(u_1) = \frac{7}{3}v_1-\frac{1}{3}u_2$$ , $$L(u_2) = -\frac{8}{3}u_1 + \frac{5}{5}u_2$$

>   $$  $$

>   $$\Rrightarrow matrix \ B=\left(\begin{array}{ccc}\frac{7}{3}&\frac{8}{3}\\ \frac{1}{3}&\frac{5}{3}\end{array}\right)$$

$$  $$

The Columns of $$ B $$ are $$(U^{-1}Au_1,U^{-1}Au_2) = U^{-1}(Au_1,Au_2)$$ , $$ B = U^{-1}A U$$
	
###Definition:-
  If $$A$$ and $$B$$ are $$n\times n$$ matrices, and $$S$$ is not singular such that $$B = S^{-1}AS$$, then $$B$$ is similar to $$A$$
	
Since $$B = S^{-1}AS$$
$$SBS^{-1} = SS^{-1} A SS^{-1}\implies A = SBS^{-1} \Rightarrow A$$ is similar to $$B$$ 

So, if $$ B $$ is Similar to $$ A $$, $$ A $$ is Similar to $$ B $$

<!--BREAK-->

####Ex)  
>   Let $$D$$ be the differential operation on $$P_3$$ Find $$B$$ representing  $$D$$ W.R.T $$[1, x, x^2]$$ and $$A$$ representing $$D$$ W.R.T. $$[3, 4x, 2x^2+x]$$

>   Since

>	$$D(1) = 0 = 0 \cdot 1 + 0 \cdot x+ 0 \cdot x^2$$

>	$$D(x) = 1 = 1 \cdot 1+0 \cdot x + 0 \cdot x^2$$

>	$$D(x^2) = 2x = 0 \cdot 1+2 \cdot x + 0 \cdot x^2$$

>  $$B = \left(\begin{array}{ccc}0&1&0\\0&0&2\\0&0&0\end{array}\right)$$

>  $$  $$

>  Transition from $$[3, 4x, 2x^2=X]$$ to $$[1, x, x^2]$$ is, 
	
>   $$D(3)   = 0 =  0 \cdot 3 + 0 \cdot 4x + 0 \cdot (2x^2+x)$$

>   $$D(4x) = 4 = \frac{4}{3} \cdot 3 + 0 \cdot 4x + 0 \cdot (2x^2+x)$$

>   $$D(3)   = 4x+1 = \frac{1}{3} \cdot 3 + 0 \cdot 4x + 0 \cdot (2x^2+x)$$
	
>   $$ A = \left(\begin{array}{ccc} 0 & \frac{4}{3} & \frac{1}{3} \\ 0 & 0 & 1 \\ 0 & 0 & 0\end{array}\right)$$
	
>   $$  $$

	
>   To go from $$[3, 4x, 2x^2+x]$$ to $$[1, x, x^2]$$ , 
$$ S = \left(\begin{array}{ccc} 3 & 0 & 0 \\ 0 & 4 & 1 \\ 0 & 0 & 2 \end{array}\right)$$ ,  $$ S^{-1} = \left(\begin{array}{ccc} \frac{1}{3} & 0 & 0 \\ 0 & \frac{1}{4} & -\frac{1}{8} \\  0 & 0 & \frac{1}{2} \end{array}\right)$$ and $$S^{-1}BS=A$$

>   $$  $$

>   $$ A $$ is  a matrix representation W.R.T.  $$[1, x, x^2]$$

>   $$ B $$ is a matrix representation W.R.T. $$[3, 4x, 2x^2+x]$$
	
>   $$ S $$ is a transitional matrix from $$[3, 4x, 2x^2+x]$$ to $$[1, x, x^2]$$.

<!--BREAK-->

## Orthogonality in $$\mathbb{R}^2$$ and $$\mathbb{R}^n$$

2 vectors are orthogonal vectors at right angles ($$\mathbb{R}^2$$ and $$\mathbb{R}^3$$)

Suppose 
$$X = (x_1, x_2, \dots ,x_n)^T $$ , $$Y = (y_1, y_2, \dots , y_n)^T$$

From 1.5 the inner product $$X^TY$$ is 

$$X^TY = x_1y_1+x_2y_2+ \dots + x_n y_n = \sum\limits^{n}_{i=1}x_i y_i$$

in $$\mathbb{R}^2$$ and $$\mathbb{R}^3$$. We call this the Dot Product.
	
Eg)
	
>   $$X = \left(\begin{array}( 1 \\ 2 \\ 3 \end{array}\right)$$ ,  $$Y = \left(\begin{array}( 3 \\ 1 \\ -4 \end{array}\right)$$

>   $$  $$

>   $$X^TY = 1 \cdot 3 + (-2) \cdot 1 + 1 \cdot (-4) = -3$$
	
>   and
	
>   $$X = \left(\begin{array}( 1 \\ 2 \\ 3 \\ 4 \end{array}\right), Y=\left(\begin{array}( 4 \\ -1 \\ 1 \\ 2 \end{array}\right) \ X^TY = 1 \cdot 4 + 2 \cdot (-1) + 3 \cdot 1+ 4 \cdot 2 = 13$$


$$  $$


##Scalar Dot Product in $$\mathbb{R}^2$$ and $$\mathbb{R}^3$$
	
Since 

$$X^TX = (x_1, x_2)\left(\begin{array} {ccc}x_1\\x_2\end{array}\right) = x^2_1+x_2^2$$

$$  $$
	
$$||x||=\sqrt{X^TX}$$ in $$\mathbb{R}^3, ||X||=\sqrt{x_1^2+x_2^2+x_3^2}$$


For $$X,Y$$in $$\mathbb{R}^2$$ or $$\mathbb{R}^3$$ where $$\theta$$ is an angle between $$Z$$ and $$Y$$

$$  $$

$$  $$

<!--BREAK-->
	
##Law of Cosines $$X^T Y=||X|| ||Y||cos\left( \theta \right)$$
	
$$||X-Y||^2=||X||^2+||Y||^2-2||X|| ||Y||cos(\theta)$$

$$\implies ||X|| ||Y||cos(\theta)=\frac{1}{2}[||X||^2+||Y||^2-||X-Y||^2]$$

$$||X||||Y||cos(\theta) = \frac{1}{2}(X^TX+Y^TY-(X-Y)^T(X-Y))=\frac{1}{2}(X^TX+Y^TY-X^TX-Y^TY+X^TY+Y^TX)$$

$$=\frac{1}{2}(X^TY+Y^TX)$$

Since $$X^TY=Y^TX$$
$$=\frac{1}{2}(X^TY+X^TY)=\frac{1}{2}(2X^TY)=X^TY$$
	
A Unit vector in the $$X$$ direction $$U=\frac{X}{||X||}$$

$$||U|| = \left|\left|\frac{X}{||X||}\right|\right| = \frac{||X||}{||X||}=1$$

and

$$cos(\theta)=\frac{X}{||X||} = \frac{||X||}{||X||}=1$$

$$u=\frac{X}{||X||}, v=\frac{Y}{||Y||}, cos(\theta)=\frac{X^TY}{||X||||Y||}=\left(\frac{X}{||X||}\right)^T\left(\frac{Y}{||Y||}\right)^T=U^TV$$

$$  $$

$$  $$

## Cauchy - Schwartz Inequality
	
If X and Y are in $$\mathbb{R}^2$$ or $$\mathbb{R}^3$$ then 

$$|X^TY| \le\|X\| \|Y\|$$ 

with equality IFF $$x=0$$ or $$ y=0$$ or $$y=\alpha x$$
 
$$theta = \frac{\pi}{2}\Rrightarrow cos(\theta ) = \theta$$
	
2 vectors $$ X $$ and $$Y$$ $$\theta $$ $$\mathbb{R}^2$$ or $$\mathbb{R}^3$$ are called Orthogonal IFF $$X^TY = 0$$

EX)

>   $$\left(\begin{array}{ccc} 1 \\ 1 \end{array}\right) \ \left(\begin{array}{ccc} 1\\-1\end{array}\right) \implies (1, 1)^T \left(\begin{array}{ccc}1\\-1\end{array}\right)=1-1=0$$

## Scalar and vector projections let X and Y be in $$\mathbb{R}^2$$ and $$\mathbb{R}^3$$

$$\alpha = \|X\|cos(\theta) = \frac{X^TY}{\|Y\|} $$
	
is called Scalar Projection. 

### Projection vector 

$$P=\alpha\frac{Y}{\|Y\|} = \left(\frac{X^TY}{Y^TY}\right)y $$


$$  $$

$$  $$

## Orthoganalaty in $$\mathbb{R}^n$$

Let $$X $$ in $$\mathbb{R}^n$$ define Euclidian norm ( length ).
 
$$ \|X\|_2 = \sqrt{X^TX} = \sqrt{x_1^2 + x_2^2 + \dots + x_n^2}$$
	
We define the angle between 2 vectors $$X$$ and $$Y$$

$$cos(\theta) = \frac{X^TY}{\|X\|\|Y\|}$$ 

$$0\leq \theta \leq \pi$$
	
Define: 2 Vectors $$X $$ ,  $$Y $$ in $$ \mathbb{R}^n$$ are called orthogonal if $$X^TY = 0$$,
 
	
Notation is $$X \ \dot \ Y $$

####EX)

>   Find the point on $$Y = 3X$$, closest to the point $$(2, 1)$$

>   $$ V = \left(\begin{array}{ccc}2\\1\end{array}\right), \ \  W = \left(\begin{array}{ccc}1\\3\end{array}\right)$$

>   ( When $$ x $$ is 1 $$ y $$ is 3 )

>   $$Q = \left(\frac{Y^T W}{W^T W}\right) w = \frac{5}{10}\left(\begin{array}{ccc} 1 \\ 3 \end{array}\right)  = \left( \begin{array}{ccc}\frac{1}{2} \\ \frac{3}{2}\end{array}\right)$$


$$  $$
	
Ex) 
>   $$  $$Find the equation of a plane containing point $$(4, 1, -3)$$, perpendicular to the normal $$N=(1, 2, 4)  P_0 = (4, 1, 3),  P = (x, y, z)$$

>   $$\vec{P_0P}  = (x-4, y-1, 2-(-3))^T =(x-4, y-1, 2+3)$$

>   p in the plain 

>   $$\implies\vec{P_0P^T} N =0 \implies (x-4, y-1, 2+3)\left(\begin{array}{ccc}1\\2\\4\end{array}\right)=0\implies (x-4)+2(y-1)+4(2+3) = 0$$

$$  $$

In general a plain contains the point $$(x_0, y_0, z_0)$$ normal to $$N = (a, b, c)$$ is given by:

$$a(x-x_0)+b(y-t_0)+c(z-z_0) = 0$$
	
<!--BREAK-->
	
EX)
>    $$X=\left(\begin{array}{c} 1 \\ 2 \\ 1\\ -2 \end{array}\right), Y = \left(\begin{array}{c} 2 \\ 1 \\ 2\\ 1 \end{array}\right)$$

>    $$  $$

>    $$cos(\theta)=\frac{X^TY}{\|X\|\|Y\|} = \frac{2+2+2-2}{\sqrt{10} \sqrt{10}} = \frac{4}{10}=\frac{2}{3}$$

>    $$cos(\theta) = \frac{2}{3} \ \ \theta = acos\left(\frac{2}{3}\right)$$

Were $$0\le \theta \le \pi$$
	
$$  $$

$$  $$

<!--BREAK-->

#Lecture notes 2014/01/29

##Similar Matries

Consider $$L:V-v$$   $$ d $$ in $$ (v)=n $$

Matrix representation depends on an ordered basis

Ex)
>   $$  L: \mathbb{R}^2 \implies \mathbb{R}^2$$ were $$ L(x)=(x-x_1, 3x_2)^T $$

>   $$ L(e_1) = \left(\begin{array}{c} 1 \\ 1 \end{array}\right)$$ , $$ L(e_2)=\left(\begin{array}{c} -1 \\ 3 \end{array}\right) $$

>   The matrix representation with respect to $$ \{e_1, e_2\} $$ is $$ \left(\begin{array}{c}\ 1 & -1 \\ 0 & 3 \end{array}\right) $$

>   Suppose we choose $$ u_1 = \left(\begin{array}{c}1 \\ 0 \end{array}\right) $$ $$ u_2 = \left(\begin{array}{c}1 \\ 2 \end{array}\right) $$ 

>   $$ L(u_1) = Au_1 = \left(\begin{array}{c} 2 \\ -3 \end{array}\right) $$ , $$ L(u_1) = Au_1 = \left(\begin{array}{c} -1 \\ 6 \end{array}\right) $$ 

>   We need $$ L(u_1) = au_1 + cu_2 $$, $$ L(u_2) = bu_1 + du_2 \implies \left(\begin{array}{c} a & b \\ c & d \end{array}\right) $$ the transistion form
$$ [u_1, u_2] = [e_1, e_2] $$ if $$ (v_1  v_2) = \left( \begin{array}{c} 1 & 1 \\ -1 & 2 \end{array}\right)$$ the transform form $$ [e_1, e_2] \rightarrow [u_1, u_2] $$ is 
$$ U^{-1} = \left(\begin{array}{c} \frac{2}{3} & \frac{-1}{3} \\ \frac{1}{3} & \frac{1}{3}\end{array}\right) $$

>   $$  $$

>   $$ U_1^{-1}L(u_1) = U^{-1}Au_1 =  \left(\begin{array}{c} \frac{7}{3} \\ \frac{-1}{3}\end{array}\right) $$ , $$ U^{-1}L(u_2) = U^{-1}Au_2 = \left(\begin{array}{c} \frac{-8}{3} \\ \frac{5}{3}\end{array}\right) $$

>   $$  $$

>   $$ L(u_1) = \frac{7}{3}v_1 - \frac{1}{3}v_2 $$ , $$ L(u_2) = \frac{-8}{3}u_1 + \frac{5}{3}u_2$$

>   Matrix 

>   $$ B = \left(\begin{array}{c} \frac{7}{3} & \frac{-8}{3} \\ \frac{-1}{3} & \frac{5}{3}\end{array}\right) $$

>   $$  $$

The columns of B are $$ (U^{-1}Av_1 , \ U^{-1}Au_2) = U^{-1}(Au_1, \ Au_2) \ \therefore $$ 

$$B = U^{-1}AU$$

### Definition :-

*If $$ A $$ and $$ B $$ are two $$ n\times n $$, and $$ S $$ is  singular such that $$ B = SBS^{-1} $$ then $$ B $$ is similar to A Since $$ B = S^{-1}AS SBS^{-1} = SS^{-1}ASS^{-1} \Rightarrow A = SBS^{-1} \implies A$$ is similar to $$ B $$ So if $$ B $$ is similar to $$ A $$ and $$ A $$ is similar $$ B $$* 
 

EX) 
>   Let $$ D $$ be the differentiation  operation on $$ P_3 $$ find $$ B $$ representing $$ D $$ W.R.T. $$ [ \ 1, \ x, \ x^2 \ ] $$ and $$ A $$ represtnting $$ D $$ W.R.T. $$ [ \ 3, \ 4x, \ 2x^2 + x \ ] $$ Since 

>>   $$ D(1) = 0 = 0 * 1 + 0 * x + 0*x^2  $$

>>   $$ D(2) = 1 = 1*1 + 0 * x + 0 * x^2  $$

>>   $$ D(3) = 2x = 0*1 + 2 * x + 0 * x^2 $$

>   $$ B =  \left(\begin{array}{c} 0 & 1 & 0 \\ 0 & 0 & 2 \\ 0 & 0 & 0 \end{array}\right) $$

>   The transition from $$ [ \ 3, \ 4x, \ 2x^2 + x \ ] $$ to $$ [ \ 1, \ x, \ x^2 \ ] $$ is.

>>   $$ D(3) = 0 = 0*3 + 0*4 + 0*(2x^2 + x) $$

>>   $$ D(4x) = 4 = \frac{4}{3} + 0*4x + 0*(2x^2 + x) $$

>>   $$ D(2x^2 + x) = 4x + 1 = \frac{1}{3}*3 + 1*4x + 0(2x^2 +x) $$

>   $$ A = \left(\begin{array}{c} 0 & 0 & 0 \\ 0 & 4 & 1 \\ 0 & 0 & 2 \end{array}\right) $$ 

>   $$  $$

>   $$ B $$ is the matrix representation W.R.T. $$ [ \ 1, \ x \ x^2] $$ A is the matrix representation W.R.T. $$ [ \ 3, \ 4x, \ 2x^2 + x \ ] $$ $$ S $$ is the transition matrix from $$ [ \ 3,  \ 4x, \ 2x^2 + x \ ] $$ to $$ [ \ 1,  \ x, \ x^2 \ ] $$

$$  $$

$$  $$

<!--BREAK-->

#Class Notes 20140212

##Orthogonal Subspaces

Suppose $$ A $$ is $$ m \times n $$ and $$ x \ \epsilon  \ N(A) \implies Ax = 0$$ the $$ a_{i1}x_1 + 2_{i2}x_2 + \dots + a_{in}x_n = 0 \implies x $$ is orthogonal to the $$ i^{th} $$ of $$ A^T $$, so if $$ x $$ is orthogonal to the $$ i^{th} $$ columns of linear combinations of the columns of $$ A $$ we say $$ R(A^T) $$ and $$ N(A) $$ are orthogonal. 

###Definition :-

 *If $$ X $$ and $$ Y $$ are two subspaces of $$  \ \mathbb{R}^n $$ we say $$ X $$ is orthogonal to $$ Y $$ if and only if $$ \ X^TY= 0 \ \ \forall \ x \ \in \ X  $$ and $$ y \ \in \ Y $$*

$$  $$


####EX) 
>  Suppose $$ X $$ is spanned by $$ e_1 $$ and $$ Y $$ is spanned by $$ e_3 $$

>  $$ \implies \ x \ \in \ X $$  if and only if $$ x = \alpha \ \left(\begin{array}{c} 1 \\ 0 \\ 0 \end{array}\right) $$,
 $$  \ y \ \in \ Y  $$ if and only if $$ Y = \beta \ \left(\begin{array}{c} 0 \\ 0 \\ 1 \end{array}\right) $$,
 $$ X^TY = \alpha \ \left(\begin{array}\ 1 \ &  0 \ & 0\end{array}\right) \cdotp \ \beta \ $$ $$\left(\begin{array}{c} 0 \\ 0 \\ 1 \end{array}\right)  = \alpha \ \beta \ \left[\left(\begin{array}{c} \ 1 \ 0 \ 0 \end{array}\right) \left(\begin{array}{c} 0 \\ 0 \\ 1 \end{array}\right) \right]= 0$$

###Definition :-
 *Let $$ Y $$ be a subspace of $$ \mathbb{R}^n $$. The set of vectors in $$ \mathbb{R}^n $$ orthogonal  to $$ Y $$ are called the "Orthogonal" and "Compliment" of $$ Y $$, denoted $$ Y^{\perp} $$*


####EX)

>   Concider $$ \mathbb{R}^5 $$ were $$ X= span(e_1, e_2)) $$, $$ Y = span(e_3, e_4) $$ if $$ X \ \epsilon Y \ \implies\left(\alpha, \beta, 0, 0, 0 \right) $$ and $$ Y \ \epsilon Y \ \implies \left( 0, 0, \delta, \ \gamma, 0 \right) $$ $$ X^TY = 0 \implies X $$ and $$ Y $$ are orthogonal, but they are not Orthogonal Compiment                                                                                                                                                                                                     
  
>   $$ X^\perp = span(e_1, e_4, e_5) $$, $$ \frac{1}{Y} = span(e_1, e_2, e_5) $$
$$ \ne Y \ \ne X$$

$$  $$


## A Big Deal *Fundamental Subspaces*: 
Let $$ A $$ be an $$ m\times n $$ matrix range $$ R(A) = \{ b \in \mathbb{R}^n \ | \ b = Ax $$ for some $$ X \in \mathbb{R}^n \} = $$ Column space of $$ A \ (S \ \cdotp S \ \cdotp\mathbb{R}^m) $$

$$ R(A) = \{ y \in \mathbb{R}^n \ | \ Y=A^T $$ for some $$ X=\mathbb{R}^n \}$$

$$ S \ \cdotp S$$ of $$ \ \mathbb{R}^n $$

$$ N(A)=\{X\in \mathbb{R}^n \ | \ Ax = 0\} $$

$$ N(A^T)= \{Y \in \mathbb{R}^m \ | \ A^TY = 0\} $$

<!--BREAK-->

###Therom 5.2.1
Find the subspace theorem:- _If $$ A $$ is any $$ m\times n $$ Matrix_ 

$$ N(A)=(R(A^T))^\perp $$

$$ N(A^T) = (R(A))^\perp $$

Ex)
>   Let $$ A = \left(\begin{array}{c} 4 & 0 \\ 1 & 0 \end{array}\right) $$ and the Column space $$ \alpha \left(\begin{array}{c} 4 \\ 1\end{array}\right) + \beta\left(\begin{array}{c} 0 \\ 0 \end{array}\right) = \alpha \left(\begin{array}{c}4 \\1\end{array}\right)$$

>   $$ R(A)= \left\{ b \ | \ b = x_1 \left(\begin{array}{c}4 \\1\end{array}\right) \right\} $$ , 
>   $$ N(A^T)=\left\{ Y \ | \ A^TY = 0\right\} $$

>   $$ \left(\begin{array}{c} 4 & 1 \\ 0 & 0 \end{array}\right)\left(\begin{array}{c}Y_1 \\ y_2 \end{array}\right)= \left(\begin{array}{c}0 \\ 0 \end{array}\right)\Rightarrow Y = \beta \left(\begin{array}{c} 1 \\ 4 \end{array}\right)$$

>   $$ X^TY = \alpha \left(\begin{array}{c} 4 & 1 \end{array}\right) \cdotp \beta \left( \begin{array}{c} 1 \\ -4 \end{array} \right) = \alpha \ \beta \cdotp 0 = 0 $$
>   $$ \Rightarrow N(A^T)= \left[R(A)\right]^\perp $$ show true for $$ N(A) = \left[ R(A^T) \right]^ \perp $$

###Theorem 5.2.2
If $$ S $$ is a subspace of $$ \mathbb{R}^n $$ then $$ Dim(S) + Dim(S^\perp) = n $$. Also if $$ \{x_1, x_2, \dotsc,x_n\} $$ is a basis of $$ S $$ then $$ \{x_{n+1}, x_{n+2}, \dotsc , x_n \}  $$ is a basis for $$ S $$ then $$ \{x_1, x_2, \dotsc, x_n\} $$ is a basis for $$ \mathbb{R}^n $$

###Definition:-
The direct sum $$ W = U \oplus V: $$ given 2 subspaces $$ U $$ and $$ V $$ of $$ W $$ such that $$ W=U+V $$ for any $$ u=U $$ and $$ v=V $$

Ex)
>   Let $$ W=\mathbb{R}^3, u=span(e_1), v=span(e^2) $$ then $$ w \in W \iff W = u\oplus v $$

###Theorem 5.2.3
   $$ w=\alpha \ e_1 \ + \ \beta \ e_2 = span(e_2, \ e_2) $$ then $$ S $$ in a subspace of $$ \mathbb{R}^n $$ then $$ \mathbb{R}^n = S\oplus S^\perp $$

####ex)

>   $$ S = span (e_1), S^\perp = span(e_2, e_3) $$ then $$ S \oplus S^\perp = span(e_1, e_2, e_3) = \mathbb{R}^3 $$

###Theorem 5.2.4
If $$ S $$ is  subspace of $$ \mathbb{R}^n $$ then $$ (S^\perp) = S $$

<!--BREAK-->

#### Ex)
>   Let $$ A = \left(\begin{array}{c} 1 & 2 & 1 \\ 0 & 1 & 2 \\ 1 & 1 & -1 \end{array}\right) $$ find a basis for $$ N(A), R(A^T), N(A^T), R(A) $$. To find $$ N(A) $$ and $$ R(A^T) $$ we have to row reduce $$ A $$

>   $$  $$

>   $$ \implies  \left(\begin{array}{c} 1 & 0 & -3 \\ 0 & 1 & 2 \\ 0 & 0 & 0 \end{array}\right) $$ 

>   $$  $$
   
>   $$N(A)\implies x_1 = 3x_3, \ x_2 = -2x_3 $$

>   $$ x \in N(A) \iff X = \alpha \left(\begin{array}{c} 3 \\ -2 \\ 1 \end{array}\right) , \ R(A^T)$$ is spaned by $$ \left(\begin{array}{c}1 \\ 0 \\ -3\end{array}\right)\cdotp\left(\begin{array}{c} 0 \\ 1 \\ 2 \end{array}\right) $$

>   $$  $$

>   $$Y\in R(A^T)Y = \beta \ \left(\begin{array}{c} 1 \\ 0 \\ -3 \end{array}\right) + \delta \ \left(\begin{array}{c} 0 \\ 1 \\ 2 \end{array}\right)  $$

>   $$ X^TY = 0 $$ 
>>   $$ Dim(N(A)) = 1 $$

>>   $$ Dim(R(A^T)) = 2 $$

>>   $$ Dim(N(A)) + Dim(R(A^T)) = 3 $$ 

>   $$ A^T = \left(\begin{array}{c} 1 & 0 & 1 \\ 2 & 1 & 1 \\ 1 & 2 & -1 \end{array}\right) \rightarrow \left(\begin{array}{c} 1 & 0 & 1 \\ 0 & 1 & -1 \\ 0 & 0 & 0 \end{array}\right) $$ 

>   $$  $$

>>   So 
   
>   $$ X \in R(A) = \alpha \left(\begin{array}{c}1 \\ 0 \\ 1 \end{array}\right) + \beta \ \left(\begin{array}{c} 0 \\ 1 \\ -1 \end{array} \right) $$

>   $$   $$

>   $$ Y \in N(A^T) = \delta \ \left(\begin{array}{c} -1 \\ 1 \\ 1 \end{array}\right) $$

>   $$  $$

>   $$ X^TY =0 \implies N\left(A^T\right) = \left[R(A)\right]^\perp$$


<!--BREAK-->

# Class notes 20140226
## Orthoganal Projectoions

Let $$ W $$ be a subspace of $$ \mathbb{R}^n $$ and $$ dim W = k, $$ $$ W = span\{x_1, x_2, \dots,x_k\} $$
 $$  $$  $$ \hat{v} \in W $$closest to $$ v $$

![Orthogonal Projection](file:///Users/JimW/Desktop/OrthProject.tif)

Answer: Want $$ \hat{v} \in W $$ so that $$ v-\hat{v} \in W^\perp $$ 

$$ \hat{v}=  $$ is the Orthoganal projection of $$ V $$ into $$ W $$ We need $$ \hat{v} $$ so that 

$$ V-\hat{v} \perp x_1 \implies (V-\hat{v}) \cdot x_1 = 0 \\ V-\hat{v} \perp x_2 \implies (V-\hat{v}) \cdot x_2 = 0 $$ 

$$   \vdots  $$ 

$$ V-\hat{v} \perp x_n \implies (V-\hat{v}) \cdot X_n = 0$$

$$ X = \pmatrix{ x_1  \cr x_2 \cr \vdots \cr x_n} Y =  \pmatrix{ y_1  \cr y_2 \cr \vdots \cr y_n}  \ \ \ \ X \cdot Y = X^T Y \ \ \ = \pmatrix{ x_1  \ x_2  \cdots  x_n} \pmatrix{ y_1  \cr y_2 \cr \vdots \cr y_n} = \sum_{i=1}^{n} X_n\cdot Y_n$$

$$ X_1^T(V-v^n)=0 \implies x_1^TV-X_1^T\hat{v}=0\implies X_1^TV=X_1^\hat{v}$$ 

$$ \vdots $$

$$ X_k^T(V-v^n)=0 \implies x_k^TV-X_k^T\hat{v}=0\implies X_k^TV=X_k^\hat{v} $$

$$ \pmatrix{ x_1^T  \cr x_2^T \cr \vdots \cr x_k^T } \pmatrix{ \cr \cr V \cr \cr }=\pmatrix{ x_1^T  \cr x_2^T \cr \vdots \cr x_k^T } \pmatrix{ \cr \cr \hat{v} \cr \cr }$$

Let $$ A=\pmatrix{ X_1 & | &  X_2 & | & \cdots & | & X_k } $$ $$  $$ $$ A^TV=A^T\hat{v} $$

col Space $$ (A)=W $$ $$ A^T $$ is $$ n \times k $$ and now invertable $$ A $$ is a $$ n\times k $$ Matrix $$ \hat{v} \in W $$ So $$ \hat{v}=c_1x_1+c_2x_2+\cdots c_k x_k $$

$$ c_1, c_2, \cdots , c_k \in \mathbb{R} $$

$$ \hat{v}= \pmatrix{ X_1 & | &  X_2 & | & \cdots & | & X_k } \pmatrix{ c_1 \cr c_2 \cr \vdots \cr c_k }= AC \therefore A^T = A^T\hat{v}=A^TAC $$

$$ A^TA $$ is $$ n\times k $$ and $$ k \times n \implies A^TA\rightarrow k \times k $$ and square, so invertible.

###Prop
Since $$ x_1, \cdots , x_k $$ are independent, $$ A^TA $$ has an inversion.  

$$ A^TV=A^TAC \implies (A^TA)^{-1}A^TV=C$$

###Formula For Projection

$$ W=span\{x_1,\cdots , x_k\} $$ $$ X_1, \cdots x_k $$ are independent. $$ V\in \mathbb{R} $$ the projection of $$ v $$ onto $$ W=\hat{V}=A(A^TA)^{-1}A^TV $$

$$ Q_w = $$ Projectio matrix onto $$ W $$

[URL for Lectures](faculty.uml.edu/dklain/projections.pdf)

If $$ \pmatrix{ A \cr \vdots \cr A^n  }\pmatrix{ x \cr y } = (?) $$ an over determined system

If $$ Ax=d $$ has no solutions, what's the best approximation for a solution? If $$ x_1A_1+\cdots x_nA_n $$ can't solve $$ Ax=b $$, try $$ A^TAv=A^Tb $$

set $$ x=(A^TA)^{-1}A^Tb $$ The least squares approximation 

>####e.g.
Compute the projection matrix $$ Q $$ for the 2D subspace $$ W $$ of $$ \mathbb{R}^4 $$ spanned by $$ (1, 1, 0, 2) $$ and $$ (-1, 0, 0, 1) $$
What id the orthogonal projection of $$ (0, 2, 5, -1) $$ onto $$ W $$

>> $$  $$$$ A = \pmatrix{ 1 & -1 \cr 1 & 0 \cr 0 & 0 \cr 2 &1 }$$

>> $$  $$$$ A^TA = \pmatrix{ 1 & 1 & 0 & 2 \cr -1 & 0 & 0 & 1 }\pmatrix{1 & -1 \cr 1 & 0 \cr 0 & 0 \cr 2 &1 } = \pmatrix{ 6 & 1 \cr 1 & 2 } $$

>> $$  $$$$ (A^TA)^{-1} = \frac{1}{det(A)} = \frac{1}{12-1}\pmatrix{2 & -1 \cr -1 & 6} = \pmatrix{ \frac{2}{11} & \frac{-1}{11} \cr \frac{-1}{11} & \frac{6}{11} }  = \frac{1}{11}\pmatrix{ 2 & -1 \cr 1 & 6 }$$

>> $$ Q = \frac{1}{11}\pmatrix{ 1 & -1 \cr 1 & 0 \cr 0 & 0 \cr 2 & 1}\pmatrix{ 2 & -1 \cr -1 & 6 }\pmatrix{ 1 & 1 &0 & 2 \cr -1 & 0 & 0 & 1 } $$ 
>> $$ =\frac{1}{11}\pmatrix{ 1 & -1 \cr 1 & 0 \cr 0 & 0 \cr 2 & 1 } \pmatrix{ 3 & 2 & 0 & 3 \cr -7 & 1 & 0 & 4 } = \frac{1}{11}\pmatrix{ 10 & 3 & 0 & -1 \cr 3 & 2 & 0 & 3 \cr 0 & 0 & 0 &0 \cr -1 & 3 & 0 & 10 }$$ 

> $$  $$

>> b) $$ Q\pmatrix{ 0  \cr 2 \cr 5 \cr -1 } = \frac{1}{11}\pmatrix{ 7 \cr 1 \cr 0 \cr -4 } $$

$$ Q_wQ_w = A\overbrace{(A^TA)^{-1}A^TA}^{I}(A^TA)^{-1}A^T \implies Q_w^2A(A^TA)^{-1}A^T = Q_w$$

It is Indempotant. 

###Line fit with least squares 

Fitting a line to data as the best fit we can. 

![Scatter Plot](file:///Users/JimW/Documents/UMassLowell/classes/LinearAlgebra/scaterplot.tiff) Wishful thinking, we need $$ y = ax+b $$ as a best fit

$$ \matrix{ y_1 = mx_1 + b  \\ y_2 = mx_2 + b \\ \vdots \\ y_n= mx_n + b } \implies \pmatrix{ y_1 \\ y_2 \\ \vdots \\ y_3} = \pmatrix{ x_1 & 1 \cr x_2 & 1 \cr \vdots & \vdots \\ x_n & 1 }\pmatrix{ m \\ b } $$

$$ Y=AC $$ Has no solution so multiply both sides by $$ A^T $$ giving $$ A^TY = A^TAC \implies (A^TA)^{-1}A^TY = C = \pmatrix{ m \\ b } $$ we have

$$ A^TA = \pmatrix{ x_1 & \cdots & x_n \cr 1 & \cdots & 1 }\pmatrix{ x_1 & 1 \cr \vdots & \vdots \cr x_n & 1 }=\pmatrix{ \sum x_i^2 & \sum x_i \cr \sum x_i & n } $$

####E.X.

> We have a data set $$ (1,3),(2,2),(3,0),(4,-1),(5,-3) $$ find the Best fit line. 

>![Least Squares fit](file:///Users/JimW/Documents/UMassLowell/classes/LinearAlgebra/leastSqEg.tiff)
>$$ \pmatrix{ 3 \\ 2 \\ 0 \\ -1 \\ -3 } \matrix{ ? \\ = \\ No!}\pmatrix{ 1 & 2 \cr 2 & 1 \\ 3 & 1 \\ 4 & 1 \\ 5 & 1 }\pmatrix{ w \\ b }\longrightarrow $$ Least Squares. 

> $$ \pmatrix{ m \\ b } \approx (A^TA)^{-1}A^TY$$

> $$ A^TA = \pmatrix{5^2+4^2+3^2+2^2+1^2 & 5+4+3+2+1 \\ 5+4+3+2+1 & 5} = \pmatrix{55 & 15 \\ 15 & 5} $$

> $$ (A^TA)^{-1} = \frac{1}{ 5(55)-15^2 } \pmatrix{ 5 & -15 \\ -15 & 55 }  = \frac{1}{50} \pmatrix{5 & -15 \\ -15 & 55} = \pmatrix{ \frac{1}{10} & \frac{-3}{10} \\  \frac{-3}{10} & \frac{11}{10} } $$

> $$ \pmatrix{ \frac{1}{10} & \frac{-3}{10} \cr \frac{-3}{10} & \frac{11}{10} }\pmatrix{ 1 & 2 &3 & 4 & 5\cr 1 & 1 & 1 & 1 & 1}\pmatrix{ 3 \\ 2 \\ 0 \\ -1 \\ -3 } = \pmatrix{ 0.1 & -0.3 \cr -0.3 & 1.1 } \pmatrix{ -12 \\ 1 } = \pmatrix{ -1.2 - 0.3 \cr 3.6 + 1.1 }  = \pmatrix{ -1.5 \\ 4.7 } $$

> $$ \therefore \matrix{ m = -1.5 \\ b = 4.7 }  \implies y \approx -1.5x + 4.7 $$


####In polynomial form. 
$$ y=ax^2+bx+c $$ we need

$$ \pmatrix{y_1 \\ y_2 \\ \vdots\\ y_n } = \pmatrix{ a_1x_1^2+b_1x_1+c  \cr a_2x_2^2+b_2x_2+c \cr \vdots \cr a_nx_n^2+b_nx_n+c } \implies \pmatrix{ y_1 \\ y_2 \\ \vdots\\ y_n } = \pmatrix{ x_1^2 & x_1 & 1 \\ x_2^2 & x_2 & 1 \\ \vdots & \vdots & \vdots \\ x_n^2 & x_n & 1} \pmatrix{ a \\ b \\ c }$$

$$ Y = AC \implies A^TY=A^TAC \implies (A^TA)^{-1}A^TY = C $$

e.x
>  $$ x_1 + x_2 = 3, \ \ 2x_1 - 3x_2 = 1, \ \ 0x_1 + 0x_2 = 2 $$ 

>  $$ \pmatrix{1 & 1 \\ 2 & -3 \\ 0 & 0} \pmatrix{ x_1 \\ x_2 } = \pmatrix{ 3 \\ 1 \\ 2 }, \ \ \ A = \pmatrix{ 1 & 1 \cr 2 & -3 \\ 0 & 0}$$ 

>  $$ \pmatrix{ 1 & 2 & 0 \\ 1 & -3 & 0} \pmatrix{ 1 & 1 \cr 2 & -3 \\ 0 & 0 } \pmatrix{ x_1 \\ x_2 }  = \pmatrix{ 1 & 2 & 0 \\ 1 & -3 & 0} \pmatrix{ 3 \\ 1 \\ 2 }$$

>  $$ \pmatrix{ 5 & -5 \cr -5 & 10 } \pmatrix{ x_1 \\ x_2 } = \pmatrix { 5 \\ 0 } \implies \pmatrix{ 1 & -1 \\ 1 & -2 } \pmatrix{ x_1 \\ x_2 } = \pmatrix{ 1 \\ 0 } $$

>  $$ \pmatrix{ x_1 \\ x_2 } = \pmatrix{1 & 1 \\ -1 & -2} \pmatrix{ 1 \\ 0 }$$

>  $$ \pmatrix{ -2 & 1 \\ -1 & 1 } \pmatrix{ 1 \\ 0 } = \pmatrix{ 2 \\ 1 } = \hat{X}$$

e.x.

>  $$ P = A \hat{X} = \pmatrix{ 1 & 1 \cr 2 & -3 \\ 0 & 0} \pmatrix{ 2 \\ 1 } = \pmatrix{3 \\ 1 \\ 0} $$

>  $$ r(\hat{X}) = b - p  = \pmatrix{ 3 \\ 2 \\ 1 } - \pmatrix{ 3 \\ 1 \\ 0} = \pmatrix{ 0 \\ 0 \\ 2} $$

>  $$ A^Tr = \pmatrix{ 1 & 1 & 0 \\ 1 & -3 & 0 \\ } \pmatrix{ 0 \\ 0 \\ 2 } = \pmatrix{ 0 \\ 0 }$$

>  $$ \implies r \in N(A^T) $$



e.x.

>  $$ A = \pmatrix{ 1 & 2 \\ 2 & 4 \\ -1 & -2 }, \ \ \pmatrix{ 3 \\ 2 \\ 1 } $$ Solve $$ A^TA\hat{x} = A^Tb $$

>  $$ \pmatrix{ 1 & 2 & -1 \\ 2 & 4 & -2 } \pmatrix{ 1 & 2 \\ 2 & 4 \\ -1 & -2 } \pmatrix{ x_1 \\ x_2 } = \pmatrix{ 1 & 2 & -1 \\ 2 & 4 & -2 } \pmatrix{ 3 \\ 2 \\ 1 } $$

>  $$ \pmatrix{ 1 & 2 \\ 1 & 2 } \pmatrix{ x_1 \\ x_2 } = \pmatrix{ 1 \\ 1 } $$

>  $$ x_2 = \alpha, x_1 = 1-2x_2 = 1-2\alpha $$

>  $$ \hat{x} \pmatrix{ 1-2\alpha \\ \alpha } = \pmatrix{ 1 \\ 0 } + \alpha \pmatrix{ -2 \\ 1 }$$ has $$ \infty \ \# $$ of solutions

e.x.

>  $$ \pmatrix{2-2\alpha \\ 1- \alpha \\ \alpha } = \hat{x} $$ 



> x  | -1| 0 | 1 | 2
> ---|---|---|---|--
> y  | 0 | 1 | 3 | 9 

>  $$ \pmatrix{1 & x_1 \\ \vdots & \vdots \\ 1 & x_n } \pmatrix{c_0 \\ c_1 } = \pmatrix{ y_1 \\ \vdots \\ y_n }  \ \ \  y = C_0 + C_1 x$$

>  $$ \pmatrix{ 1 & -1 \\ 1 & 0 \\ 1 & 1 \\ 1 & 2 } \pmatrix{ c_0 \\ c_1 } = \pmatrix{ 0 \\ 1 \\ 3 \\ 9 } $$

>  $$ \pmatrix{ 1 & 1 & 1 & 1 \\ 1 & 0 & 1 & 2 } \pmatrix{ 1 & -1 \\ 1 & 0 \\ 1 & 1 \\ 1 & 2 } \pmatrix{ c_0 \\ c_1 } = \pmatrix{ 1 & 1 & 1 & 1 \\ -1 & 0 & 1 & 2}\pmatrix{ 0 \\ 1 \\ 3 \\ 9 } $$

>  $$  \pmatrix{ 4 & 2 \\ 2 & 6 } \pmatrix{ c_0 \\ c_1 }= \pmatrix{ 13 \\ 21 } $$

>  $$  \pmatrix{c_0 \\ c_1} = \pmatrix{ 4 & 2 \cr 2 & 6 }^{-1} \pmatrix{ 13 \\ 21 } $$

>  $$ = \frac{1}{20}\pmatrix{ 6 & -2 \cr -2 & 4 }\pmatrix{ 13 \\ 21 } = ? = \pmatrix{ \frac{35}{20}  \\ \frac{38}{20} } $$

>  $$ y = C_0 + C_1 x = \pmatrix{ \frac{9}{5} \\ \frac{29}{10} } $$

>  $$ y = \frac{9}{5} + \frac{29}{10}x $$

If $$ (x_1,y_1)\dots (x_n, y_n) $$ and $$ x=( \ x_1 \ \ x_2 \ \ \dots \ \ x_n \ ) $$ and $$ y = ( \ y_1 \ \ y_2 \ \ \dots \ \ y_n \ ) $$. 
Then $$ \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i $$ and $$ \bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i $$

Let $$ y = C_0 + C_1 x $$ be a liner function of best fit, show that if $$ \bar{x}=0, C_0 = \bar{y}, C_1 = \frac{X^TY}{X^TX} $$ 

$$ \pmatrix{ 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n} \pmatrix{ c_0  \cr c_1  } = \pmatrix{y_1 \\ y_2 \\ \vdots \\ y_n } $$

$$ \pmatrix{ 1 & 1 & \dots & 1 \\ x_1 & x_2 & \dots &x_n } \pmatrix{ 1 & x_1 \cr 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n } \pmatrix{ c_0  \\ c_1  } = \pmatrix{ 1 & 1 & \dots & 1 \\ x_1 & x_2 & \dots & x_n } \pmatrix{ y_1 \\ y_2 \\ \vdots  \\ y_n }$$

$$ \pmatrix{ n & \sum_{i=1}^{n} x_i \cr \sum_{i=1}^{n} x_i & \sum_{i=1}^{n} x^2_n } \pmatrix{ c_0  \\ c_1 } = \pmatrix{ \sum_{i=1}^{n} y_1 \\ \sum_{i=1}^{n}x_iy_i }$$

$$ c_0 = \frac{1}{n}\sum_{i-1}^{n}y_1 = \bar{y}, \ \ c_1 = \frac{\sum_{i=1}^{n} x_i \ y_1}{\sum_{i=1}^{n} x_i \ x_i} = \frac{X^TY}{X^TX}$$

>  $$ P=A(A^TA)^{-1}A^T $$  for $$ A $$ is an $$ m \times n $$ rank $$ n $$ Show $$ P^2  = P$$ prove $$ P^k = P $$.

>  $$ P^2 = [A \ (A^TA)^{-1} \ A^T][A \ (A^TA)^{-1} \ A^T] $$

>  $$ = A \ (A^TA)^{-1} \ A^TA(A^TA)^{-1} \ A^T $$

>  $$  = A\ (A^TA)^{-1}A^T = P $$

>  Prove $$ P^k = P $$

>  Assume $$ P^n = P $$ show true for $$ m + 1 $$

>  $$ P \ P^m = P \cdot P $$

>  $$ P^{m + 1} = P^2 = P $$

> Show $$  P  $$ is $$ sym \implies P^T = P $$

>  $$ (A(A^TA)^{-1} \ A^T) = (A^T)^T [ A^TA]^{-1} \ A^T = A[A^T A^{T^T}]^{-1}A^T  = A[A^TA]^{-1}A^T = P$$

> $$  \implies P^T = P \implies P $$ is Symmetrical

## 5.4 Inner Product Space

Def:- 
An inner product on a vector space $$ V $$ is an operation that assosiates a real number $$ \langle X,Y \rangle $$ to each pair of vectors $$ X,Y \in V $$ such that 

1.   $$ \langle X , Y \rangle = \langle Y, X \rangle$$
2.   $$ \langle X , X, \rangle \ge 0 $$ with equality if and only if $$ x=0 $$
3.   $$ \langle \alpha X + \beta Y , 2 \rangle = \alpha\langle X,2\rangle + \beta \langle Y, 2 \rangle$$


####ex) 
1.   $$ \mathbb{R}^n \ \langle X, y \rangle = X^TY $$
2. Weighted inner product
3.   $$ \langle X, Y \rangle = \sum_{i=1}^{n} w_i $$

####1a)

>  $$ A=\pmatrix{ 3 & 4 \cr 6 & 8 }\rightarrow\pmatrix{ 3 & 4 \cr 0 & 0 }\implies R(A^t) $$ has a basis $$ \pmatrix{ 3  \cr 4  } $$

>  $$ \pmatrix{ 3 & 4 \cr 0 & 0 } \pmatrix{ x_1  \cr x_2  } = \pmatrix{ 0  \cr 0  } \implies \pmatrix{ x_1  \cr x_2  } = \pmatrix{ 4  \cr -3  }$$ 

>  $$ N(A) $$ has the basis $$ \pmatrix{ 4  \cr -3  } \ \ R(A) $$ has the basis $$ \pmatrix{ 2  \cr -1  } $$
 
> $$ N(a) $$$$ \pmatrix{ 1 & 2 \cr 0 & 0 } \pmatrix{ x_1  \cr x_2  } = \pmatrix{ 0  \cr 0  } \implies \pmatrix{ x_1  \cr x-2  } = \pmatrix{ 2  \cr -1  } $$

>  The Basis for $$ N(A^T) $$ is $$ \pmatrix{ 2  \cr -1  } $$ or $$ \pmatrix{ -2  \cr 1  } $$

####1c)

>  $$ A=\pmatrix{ 4 & -2 \cr 1 & 3 \cr 2 & 1 \cr 3 & 4 } \rightarrow \pmatrix{ 1 & 0 \cr 0 & 1 \cr 0 & 0 \cr 0 & 0 } $$ Basis for $$ R(A) $$ is $$ \pmatrix{ 1  \cr 0  } \ \pmatrix{ 0  \cr 1  } $$ 

>  $$ N(A) \ \ \ \pmatrix{ 1 & 0 \cr 0 & 1 \cr 0& 0 \cr 0 & 0 } \pmatrix{ x_1  \cr x_2  } = \pmatrix{ 0  \cr 0  } \implies \pmatrix{ x_1  \cr x_2  } = \pmatrix{ 0  \cr 0  }$$


####ex)

>  $$ e[a,b] $$

>  $$ \langle \ f, g \ \rangle = \int_{a}^{b} f \cdot g \ dx  $$ or weighted $$ \langle \ f, g \ \rangle = \int_{a}^{b} w \ f \cdot g \ dx $$ where $$ w( \ x \ )> 0 $$

Define length or norm. $$ \|v\| = \sqrt{\langle v, v, \rangle} $$

1.   $$ \langle v, v \rangle = \sum v_i^2 \ \ \ \ \|v\| = \sqrt{\sum v_i^2}$$
2.   $$ u \perp v \iff \langle u, v\rangle = 0 $$


####Thm : Pythagorean Law

$$ u \perp v \ \ \ \| u + v \|^2 = \| u \|^2 + \| v \|^2$$

$$ \| u + v \|^2 = \langle u + v , u + v \rangle = \langle u, u \rangle + \overbrace{\langle v, u \rangle + \langle u , v \rangle}^{=0} + \langle v, v \rangle = \| u \|^2 + \| v \|^2 $$


####ex)

>   $$ e[-1, 1] $$

>   $$ u = 1 \ , \ \ v = x^2 \ \ \ \ \langle 1 , x^2 \rangle = \int_{-1}^{1} 1 \cdot x^2 \ dx \ = \ 0  $$

>   length $$ \| 1 \|^2 = \langle 1, 1 \langle = \int_{-1}^{1} 1 \cdot 1 \ dx = 2  $$

>   $$ \| x^3 \| ^2 = \langle x^3 , x^3 \rangle = \int_{-1}^{1} x^3 \cdot x^2 \ dx = \frac{2}{7}  $$

>   So $$ \| 1 \| = \sqrt{2} \ \ \ \ \| x^3 \| = \sqrt{\frac{2}{7}} $$

>   One can say $$ \| 1 + x^2 \| = \frac{16}{7} = 2 + \frac{2}{7} $$


#### For $$ \mathbb{R}^{m\times n} $$ Frobeio's Norm.

$$ \| \cdot \|_f $$ where 

$$ \| A \|_f = \langle A, A \rangle^{\frac{1}{2}} = \pmatrix{ \sum_{i=1}^{n} \sum_{j=1}^{n} a_{i j}^{2} }^{\frac{1}{2}} $$

####e.g. 
$$ A = \pmatrix{ 1 & 2 & 4 \cr -1 & 3 & 2 } \ \ \ \|A\|_f = \pmatrix{ 1+4+16+1+9+4}^{\frac{1}{2}} = \sqrt{35}$$ 

Consider $$ P_n(x) $$ polynomials of degree $$ < \ n $$. Let $$ {x_1, \dots , x_n} $$ be $$ n $$ distinct numbers. Let $$ p(x), \ q(x) \in P_n(x) $$

$$ \langle p, q \rangle = \sum_{i=1}^{n} p(x_i)q(x_i) $$ 

is also the inner product. 

1.   $$ \langle P, p \rangle = \sum_{i=1}^{n} P^2 (x_i) \ge 0$$ 

2.   Since $$ \sum_{i=1}^{n} p(x)q(x_i) = \sum_{i=1}^{n} Something $$

3.   $$ \langle \alpha p + \beta q, n \rangle = \sum_{i=1}^{n} \lgroup \ \alpha \ p(x_i) + \beta \ p \ (x_i) \ \rgroup \ r(x_i) = \sum_{i=1}^{n} \matrix{ \alpha \ p(x_i) & \dots \cr \vdots } $$ 

#### e.x.

>   $$ x_i = \frac{i-1}{2}, \ \ i = 1, 2, 3, \ \ \ \ \ x_1 = 0, \ \ x_2 = \frac{1}{2}, \ \ x_3 = 1 $$

>   $$ \| 2x \| = \sqrt{\langle 2x, 2x \rangle} = \lgroup \sum_{i=1}^{n} 4x^2 \rgroup ^{\frac{1}{2}} = (0 + 1 + 4 )^{\frac{1}{2}} = \sqrt{5}$$


####Def:

If $$ u_1 v_1 \in V $$ and $$ \sqrt{ } \ne 0 $$ Then the scalar projection of $$ u $$ onto $$ v $$ is $$ \alpha = \frac{\langle u, v \rangle}{\| v \|} $$ the vector projection 

$$ P = \alpha \pmatrix{ \frac{v}{\| v \|}} = \pmatrix{ \frac{\langle u,v \rangle}{\| v \|} }^{\frac{1}{2}}$$

#### The Couchy Schwartz Inequality

let $$ u $$ and $$ v $$ be the inner product space $$ V $$, 
then $$ \lvert \langle u, v \rangle \rvert \le \| u \| \| v \|$$ with equality $$ \iff u = \alpha \ v $$ 
i.e. $$ u $$ and $$ V $$ are linearly independent. 

#### Proof

If $$ v \equiv 0 \ \ \ \ \lvert \langle u, v \rangle \rvert = \| u \| \cdot 0 = 0$$

Let $$ P $$ be the vector projection of $$ u $$ onto $$ v $$, since $$ P $$ and $$ U \cdot P $$ are orthogonal

$$ \| u - p \|^2 + \|p \|^2 = \| u \|^2$$

so

$$ \| p \|^2 = \| u \|^2 - \| u - p \|^2 = \bigg\lbrack \cfrac{\lvert \langle u, v \rangle \rvert }{\| V \|^2} \cdotp \| v \|^2 \bigg\rbrack $$

$$ \implies \lvert \langle u, v \rangle \rvert = \| v \|^2 \pmatrix{ \| u \|^2 - \| u - p \|^2 } \le \|v\|^2\|u\|^2 $$

$$ \implies \lvert \langle U, V \rangle \rvert \le \| u \| \| v \| $$

equality $$ \iff \| u - p \|^2  = 0 \implies u = p $$ either $$ v=0  $$ or $$ u $$ and $$ v $$ are in the same direction since.

$$ \lvert \langle u, v \rangle \rvert \le \|u\|\|v\| \le \langle u, v \rangle \le \|u\|\|v\|-1 \le \cfrac{\langle u, v \rangle}{\|u\|\|v\|} = \cos(\theta) $$


#### Def:-
Normalled linear vector space given a vector space $$  V  $$ if each $$ v\in V $$ has a real number .............

1.   $$ \|v\| \ge 0 \ \ \ \| v \| = 0 \iff v = 0 $$

2.   $$ \| \alpha \| = | \alpha | \|v\| \Delta $$ inequality. 

3.   $$ \| u + v \| \le \| u \| + \| v \| $$


#### Theorem. 
If $$ V  $$ is an inner ptroduct space, then 

$$  \|v\| = \langle v, v \rangle ^{\frac{1}{2}} \ \ \forall \ \ v \in V $$ 

defines a norm on $$ V $$

1.   $$ \langle v, v \rangle^\frac{1}{2} \ge 0  $$ SInce $$ \langle v, v, \rangle \ge 0 $$

2.   $$ \| \alpha v \| = \langle \alpha \ v,  \alpha \ v \rangle ^ \frac{1}{2} = \lbrack\alpha^2 \langle v, v \rangle \rbrack ^\frac{1}{2} = \pmatrix{ \alpha ^2 } ^\frac{1}{2} \ \langle v, v \rangle ^\frac{1}{2} = \lvert \ \alpha \ \rvert \langle v, v \rangle^\frac{1}{2} = \lvert \ \alpha \ \rvert \| v \|$$

3.   $$ \| u + v \|^2 = \langle u + v, u + v \rangle = \langle u, u \rangle + 2 \langle u, v \rangle + \langle v, v \rangle \le \| u \|^2 + 2 \| u \| \| v \| + \| v \| ^2 \le \pmatrix{ \| u \| + \| v \| }^2 $$

$$ \| u + v \| \le \| u \| \| v \| $$ 

Some other norms $$ V\equiv \mathbb{R}^n $$

1.   $$ \| x \|_1 = \sum_{i=1}^{n} \lvert x_1 \rvert $$

2.   $$ \|x\|_\infty = \underset{i}{\max} \lvert x_i \rvert $$

3.   P-norm $$ \| x\|_p = \bigg\lbrack \sum_{i=1}^{n} \lvert x_i \rvert ^P \bigg\rbrack ^ \frac{1}{P} $$ for $$ P = 2 \implies $$ Euclidian Norm. $$ \iff P \ne P_0 $$ Theorem won't hold. 

#### Def:-
If $$  x, y \in V $$ where $$ V $$ is a N.L.V.S, then $$  \| x - y \| \equiv $$ to the distance between $$ x $$ and $$ y $$

####e.x
 
>    Suppose $$  \langle f, g \rangle = \int_{-1}^{1} f \cdot g \ dx  \  $$  define  $$ \| \cdot \| = \langle f, g \rangle^\frac{1}{2}$$

>   if $$ f = x  $$ and $$  g = x^2 $$

>   $$ \| \ f - g \ \| = \| \ x - x^2 \ \| =  \bigg\lbrack \int_{-1}^{1} (x-x^2)(x-x^2) dx \bigg\rbrack ^\frac{1}{2}  $$ = distance between functions

## 5.5 Ortho Normal Spaces (sets)

#### Def:-
Let $$ \lbrace v_1, v_2, \dots , v_n  \rbrace  $$ be non zero vectors in an inner product space $$ V $$ if $$  \langle v_i, v_i \rangle = 0 $$ whenever $$  i \ne i $$ then the set is said to be orthogonal

####e.g.

>   $$ \pmatrix{ 1 \cr 2 \cr 1  }, \pmatrix{ 2 \cr -1 \cr 0 } ,\pmatrix{ 2 \cr 4 \cr -10 } $$ is an orthogonal set. 

#### Thm:-
If $$ \lbrace v_1,  \dots , v_n \rbrace $$ is an orthogonal set of vectors in an inner product $$  V $$, then $$ \lbrace v_1,  \dots , v_n \rbrace $$ is linearly independent. From

$$ c_1 v_1 + c_2 v_2 + \dots +c_nv_n = 0 $$

$$ \langle c_1v_1 + c_2v_2 + \dots c_nv_n, v_i \rangle = 0 $$

$$ c_1 \langle v_1, v_i \rangle + c_2 \langle v_2, v_i \rangle + \dots + c_n \langle v_n, v_i \rangle = 0$$

$$ \implies c_i \langle v_j, v_i \rangle = 0$$

$$ c_i \| v_i \|^2 = 0 $$

$$ \implies c_i = 0 $$

#### Def:-
An orthogonal set of vectors is an Orthogonal set of unit vectors. The set $$ \lbrace u_1, u_2, \dots , u_n \rbrace  $$ is orthogonal is $$  \langle u_i, u_i \rangle = \delta_ij = \cases{1, & i = j \cr 0, & i â‰  j}$$    (Kronken Delta Function)


#### Def:-
Suppose $$ B = \lbrace u_1, u_2, \dots, u_n \rbrace $$ is an orthonormal basis for $$ S $$, then $$  S = span(u-1, \dots , u_n) $$

####Perseval's Formula. 

if $$ \lbrace u_1, u_2, \cdots , u_n\rbrace $$ is an orthonormal basis for an inner product space $$ V $$ and $$ V=\sum_{i=1}^{n} c_iu_i $$ then $$  \| u \|^2 = \sum c_i^2 $$


####e.g.
>   $$ u_1 = \pmatrix{\frac{1}{\sqrt{5}} \ , \ \frac{2}{\sqrt{5}}} \ , \ u_2 = \pmatrix{\frac{-2}{\sqrt{5}} \ , \ \frac{1}{\sqrt{5}}} $$ 

>   if $$ X\in \mathbb{R}^2 $$ then $$ X^Tu_1 = \cfrac{x_1 + 2x_2}{\sqrt{5}} $$    and   $$ X^Tu_2 = \cfrac{-2x_1 + x_2}{\sqrt{5}} $$

>   $$ \| x \| ^2 =  \pmatrix{\cfrac{x_1 + 2x_2}{\sqrt{5}}} + \pmatrix{\cfrac{-2x_1 + x_2}{\sqrt{5}}} = x_1^2 + x_2^2 $$


### Orthogonal Matrices

#### Def:-
An $$ n \times n $$ matrix $$ Q $$ is called orthogonal if the Column vectors of $$ Q $$ form an orthogonal set in $$ \Rn $$ 

#### Thm:-
 An $$ n\times n $$ matrix $$ Q $$ is orthogonal $$ \iff Q^TQ\cdot I = Q^{-1} = Q^T $$ 

$$ Q =  \rotateDD  $$

$$ \bigg\| \pmatrix{\cos(\theta) \\ \sin(\theta) } \bigg\| = \cos^2(\theta)+\sin^2(\theta) = 1$$

$$ Q^T = \pmatrix{ \cos(\theta) & \sin(\theta) \cr -\sin(\theta) & \cos(\theta) } $$

$$ Q^TQ = \pmatrix{ \cos(\theta) & \sin(\theta) \cr -\sin(\theta) & \cos(\theta)} \rotateDD = $$

$$\pmatrix{ \cos^2(\theta)+\sin^2(\theta) & -\cos(\theta)\sin(\theta)+\cos(\theta)\sin(\theta) \cr -\sin(\theta)\cos(\theta)+\sin(\theta)\cos(\theta) & \sin^2(\theta)+\cos^2(\theta) } = \pmatrix{1 & 0 \cr 0 & 1} = I$$ 

If $$ Q $$
 is an $$ n\times n $$ orthogonal matrix, 

1. The Column vectors are orthogonal in $$ \Rn $$

2.    $$ Q^TQ = I $$

3.    $$ Q^{-1}  = Q^T$$

4.    $$ \langle  \ Q_x \ , \ Q_y \ \rangle = \langle  \ x \ , \ y \ \rangle $$ 

5.    $$ \| Q_x \|_2 = \| x \|_2 $$


## Least Squares. 

Solve $$ Ax = b  $$ for $$ A: m \times n $$ then $$ A^TA\hat{x} = A^Tb $$

### Thm:-
If the columns of $$ A $$ form an orthoganal set on $$ \Rn $$ then $$ A^TA = I $$ and $$ \hat{x} = A^Tb $$

#### e.x.

Find the best least squares approximation to $$ \cos (x) $$ on $$ [0, 1] $$ by a linear function. 

Let $$ S $$ be the subspace of all linear functions in $$ C[0, 1] $$, then $$ u_1 = 1 \ , \ u_2 = \sqrt{12} \ ( \ x \ - \ \frac{1}{2}) $$ is an orthogonal set of vectors. 

Let 

$$ c_1 = \int_{0}^{1} u_1 \cos(x) \ dx = 1 \\ c_2 = \int_{0}^{1} u_2 \cos(x) \ dx = \sqrt{12} \ \int_{0}^{1} \bigg(x - \cfrac{1}{2} \bigg) cos(x) \ dx $$

$$ = \bigg( \cfrac{1}{2} \ \sin(1)  + \cos(-1)  \ \bigg)x + \bigg(\cfrac{3}{4}\sin(1) - \cfrac{1}{2}\cos(1) + \cfrac{1}{2} \bigg)$$



# Class Notes 03/26/2014

Let $$ L(x) = (x_2 + x_3, x_1+x_3, x_1+x_2)^T $$ Determin the standard matrix. 

$$ L(e_1)= L\pmatrix{ 1 \\ 0 \\ 0 } = \pmatrix{0 \\ 1 \\ 1 } $$ 

$$ L(e_2)= L\pmatrix{ 0 \\ 1 \\ 0 } = \pmatrix{1 \\ 0 \\ 1 } $$  

$$ L(e_3)= L\pmatrix{ 0 \\ 0 \\ 1 } = \pmatrix{1 \\ 1 \\ 0 } \\  $$

$$ A = \pmatrix{ 0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0} $$

$$ L\pmatrix{1 \\ 3 \\ 2} = A \pmatrix{ 1 \\ 3 \\ 2 } = \pmatrix{ 0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0}\pmatrix{ 1 \\ 3 \\ 2 }= \pmatrix{ 5 \\ 3 \\ 4 }$$


Let $$ A $$ and $$ B $$ be similar matrices, $$ \lambda $$ is any scalar. Show $$ A - \lambda I $$ and $$ B - \lambda I $$ are similar.

$$ A = S^{-1}BS $$

$$ A-\lambda I = S^{-1}BS - \lambda I $$

$$ A-\lambda I = S^{-1}BS -S^{-1}A(\lambda I) = S^{-1}BS - S^{-1}(\lambda I)S $$

$$ A-\lambda I = S^{-1}(B-\lambda I)S \implies A-\lambda I $$ and $$ B-\lambda I $$ are similar



 

Let $$ A $$ be $$ n \times n $$, if there exists a nonzero vector $$ x $$, so that $$ Ax = \lambda x $$ for some scalar $$ \lambda $$, then we call $$ \lambda $$ an Eigenvalue of $$ A $$ and $$ x $$ is the associated Eigenvector (Not Unique)

#### e.x.
>   $$  A = \pmatrix{3 & -3 \\ -4 & 2 } \ , \ x = \pmatrix{-2 \\ 2} $$

>   $$ Ax = \pmatrix{3 & -3 \\ -4 & 2 }\pmatrix{-2 \\ 2} = \pmatrix{ -12 \\ 12 } = 6\pmatrix{-2 \\ 2 } = 6 x $$

>   So $$ Ax = 6x \ , \ \lambda=6 \ , \ x = \pmatrix{ -2 \\ 2 } $$ 


If $$ A $$ is $$ n\times n $$ and $$ \lambda $$ is an eigenvalue with eigenvectors $$ x $$

a. $$ \lambda $$ is an eigenvalue of a. 

b. $$ (A-\lambda I)x $$ has nontrivial solutions. 

c.   $$ N(A-\lambda ) \ne 0 $$.

d. $$ A-\lambda I  $$ is singular. 

e.   $$  \det(A-\lambda I) = 0 $$.

The last part $$  \det(A-\lambda I) \equiv n^{th} $$ order polynomial in $$ \lambda $$ is called the Characteristic Ploynomial.

#### e.g.
FInd the eigenvalues and eigenvectors for $$ A = \pmatrix{ 1 & 5 \\ 2 & -2 } $$

Form $$ (A- \lambda I) = \pmatrix{ 1-\lambda & 5 \\ 2 & -2-\lambda } $$ 

We want $$ \det (A-\lambda I) = \bigg \lvert \matrix{ 1-\lambda & 5 \cr 2 & -2-\lambda } \bigg \rvert  = 0$$

$$ (1-\lambda)(-2-\lambda) - 10 = 0 $$

$$ \lambda^2 + \lambda - 12 = 0 \implies (\lambda +4)(\lambda -3) = 0 $$

$$ \lambda = -4 \ , \ \lambda = 3 $$
to get the eigenvectors $$  x_1 , x_2 $$ solve $$ (A-\lambda I)x = 0 $$ 

$$ \lambda_1 = -4 \ , \ \bigg \lvert \matrix{ 1-(-4)  & 5 \cr 2 & -2-(-4) } \bigg \rvert  = \pmatrix{ 5 & 5 \cr 2 & 2 } x_1 = \pmatrix{ 0 \\ 0 } \implies x_1 \alpha \pmatrix{ 1 \\ -1 }$$

$$ \lambda_2 = 3 \ , \ \bigg \lvert \matrix{ 1-3  & 5 \cr 2 & -2-3 } \bigg \rvert  = \pmatrix{ -2 & 5 \cr 2 & -5 } x_2 = \pmatrix{ 0 \\ 0 } \implies x_2 \alpha \pmatrix{ 5 \\ 2 }$$

eigenspace has basis vectors $$  \pmatrix{ 1 \\ -1} \ , \ \pmatrix{ 5 \\ 2 } $$

$$ v \in  $$ eigenspace of $$ A \iff v = c_1 \pmatrix{ 1 \\ -1} + c_2 \pmatrix{ 5 \\ 2 } $$

In General

$$ A= \pmatrix{ a & b \cr c & d } $$

$$ A - \lambda I =  \bigg \lvert \matrix{ a-\lambda  & b \cr c & d-\lambda } \bigg \rvert = (a-\lambda)(d-\lambda) - bc = 0 $$ 

$$\implies \lambda^2 - \mathrm{tr}(A)\lambda + \det(A) = 0 $$

#### Thm:-
If $$ A $$ and $$ B $$ are similar, they have the same characteristic polynomials. $$ \implies $$ same eigenvalues. ( eigenvectors will be diffwerent, possibly )

### Complex Eigenvalues. 

$$ Ax = \lambda x $$ and $$ A $$ is Real and $$ \lambda  $$ is complex,

$$ \bar{Ax} = \bar{\lambda x} $$

$$ A\bar{x} = \bar{\lambda} \bar{x} $$

$$ \implies \bar{x} $$ is a eigenvector assosciated with $$ \bar{\lambda} $$

$$ \lambda = a-b\bar{\mathbb{i}} \ , \ \bar{\lambda} = a-b\mathbb{i} $$

$$ \lambda = 1 - 3\mathbb{i} \ , \ \bar{\lambda} = 1 +3\mathbb{i} $$

### Systems of constant coefficient. linear differential equations. 

In 1-D $$ y\prime = 3y $$ is a 1^st order differential equation. 

$$ y  \ \alpha  \ \mathbb{e}^{3t} \ , \  y = c\mathbb{e}^{3t}  $$

$$ y \prime = c(3\mathbb{e}^{3t}) = 3(c\mathbb{e}^{3t})  = 3y$$

In general 

$$ y \prime = a y $$ 


assume

$$ y(t)=\mathbb{e}^{\lambda t} $$

$$ \lambda \mathbb{e}^{\lambda t} \implies \lambda = a $$

The solution is 

$$ y = c \mathbb{e}^{a t} $$

#### First order systems

$$ \pmatrix{ \matrix{y_1\prime & = & a_{11}y_1 & + & a_{12}y_2 & + & \dots & + & a_{1n} y_n } \\ \matrix{ y_2\prime & = & a_{21}y_1 & + & a_{22}y_2 & + & \dots & + & a_{2n} y_n } \\ \matrix{\vdots &  \ &  \  & \vdots & \ & \ & \ & \vdots & \ & \ & \ & \ & \ & \vdots } \\ \matrix{y_n\prime & = & a_{n1}y_1 & + & a_{n2}y_2 & + & \dots & + & a_{nn} y_n } }$$

In part 2-D 

$$ y_1\prime = a y_1 + b y_2 \\ y_2\prime = c y_1 + d y_2$$

$$ A = \pmatrix{ a_{\imath \jmath} } $$

$$ Y = \pmatrix{y_1, y_2, \dots y_n}^T $$

$$ Y\prime = \pmatrix{y_1\prime, y_2\prime, \dots y_n\prime}^T $$

$$ Y\prime = A Y $$ looks like 1-D $$ y\prime = ay $$ Look for solution to 

$$ Y = \pmatrix{x_1 \mathbb{e}^{\lambda t} \cr x_2\mathbb{e}^{\lambda t} \cr \vdots \cr x_n\mathbb{e}^{\lambda t}} = \mathbb{e}^{\lambda t}x$$

Where $$ x= \pmatrix{x_1, x_2, \dots , x_n}^T $$ if $$ Y = \mathbb{e}^{\lambda t} = \lambda \mathbb{e}^{\lambda t} x $$

$$ \mathbb{e}^{\lambda t} \ne 0 $$ devide by it $$ Ax = \lambda x $$  standard eigenvalue problem

In General we solve 

$$ Y\prime = AY $$

for initial condition $$ Y(0)=Y $$

#### e.g.
$$ y_1 \prime = 2y_1 + 2y_2 \ , \ y_1(0)=3$$

$$ y_2 \prime = 2y_1 + 5y_2 \ , \ y_2(0)=1$$

So

$$ Y^\prime = \pmatrix{2 & 2 \cr 2 & 5}Y $$

for $$ Y = \pmatrix{ y_1 \\ y_2} $$ and $$ A = \pmatrix{2 & 2 \\ 2 & 5} $$

The Characteristic polynomial is

$$ \lambda^2 - 7 \lambda + 6 = 0 $$

$$ (\lambda - 6)( \lambda -1) = 0 $$

$$ \lambda_1 = 1  $$ Solve $$ (A - \lambda I)x = 0 $$

$$ \pmatrix{ 1 & 2 \cr 2 & 4 }x_1 = \pmatrix{ 0 \\ 0} \ , \ x_1 \ \alpha \pmatrix{ 2 \\ -1 } $$

$$ \lambda_2 = 6 $$ Solve $$ (A - \lambda I)x = 0 $$

$$ \pmatrix{ -4 & 2 \cr 2 & -1 }x_2 = \pmatrix{ 0 \\ 0} \ , \ x_2 \ \alpha \pmatrix{ 1 \\ -2 }$$

The solution is 

$$ Y = c_1 \pmatrix{ 2 \\ -1 } \mathbb{e}^{t} + c_2 \pmatrix{ 1 \\ 2 } \mathbb{e}^{6t} $$

$$ t = 0 \ , \ Y(0) = \pmatrix{ 2c_1 + c_2 \\ -c_1 + 2c_2 } = \pmatrix{ 3 \\ 1 }   \implies c_1 = c_2 = 1$$

$$ Y(t) = \pmatrix{ y_1(t) \\ y_2(t) } = \pmatrix{ 2 \\ -1 } \mathbb{e}^t + \pmatrix{ 1 \\ 2 } \mathbb{e}^{6t}$$


#### // insert a picture of tank mixing problem. 

> Tank A initially has 40g of salt, 

> Tank B initially has 100g of salt.

> Find the amount of salt in either tank at time $$ (t) $$

> Let $$ y_1(t) =  $$ the amount of salt in tank A

> and $$ y_2(t) =  $$ the amount of salt in tank B

> Concentration in A is $$ \dfrac{y_1(t)}{100} $$, B is $$ \dfrac{y_2(t)}{500} $$

> $$ \dfrac{dy_1}{dt} \equiv $$ of the rate of change of the salt in tank A

> $$ \dfrac{dy_2}{dt} \equiv $$ of the rate of change of the salt in tank B

> $$ \dfrac{dy_1}{dt} = - \bigg(\dfrac{y_1}{100} \bigg)(30) + \bigg(\dfrac{y_2}{500}\bigg)(10) $$

> $$ \dfrac{dy_2}{dt} =  \bigg(\dfrac{y_1}{100} \bigg)(30) - \bigg(\dfrac{y_2}{500}\bigg)(10) - \bigg (\dfrac{y_2}{500} \bigg )(20) $$

> $$ \dfrac{dy_2}{dt} = - \bigg(\dfrac{y_1}{100} \bigg)(30) - \bigg(\dfrac{y_2}{500}\bigg)(30) $$

> $$ Y_1(0) = 40g \ , \ y_2(0) = 100g $$

> Define $$ Y = \pmatrix{ y_1(t) \\ y_2(t)} $$

> $$ Y^\prime = \pmatrix{-\frac{3}{1} y_1 - \frac{1}{50} y_2 \\ \frac{3}{10} y_1 - \frac{3}{50} y_2 }$$ 

> $$ Y^\prime = AY \ , \ A = \pmatrix{-\frac{3}{10} & \frac{1}{50} \\ \frac{3}{10} & -\frac{3}{50}} $$

> $$ Y_0 = \pmatrix{ 40 \\ 100 }$$

> Assume $$ Y = x\mathbb{e}^{\lambda t} $$

> $$ \lambda^2 + \frac{18}{50}\lambda - \frac{6}{500} = 0 $$

> Charcteristic Polynomial is 

> $$ \lambda^2 + \frac{9}{25}\lambda - \frac{3}{250} = 0 \implies \lambda_1 \ , \ \lambda_2$$ 