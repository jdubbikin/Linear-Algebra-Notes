

# Lecture Notes

## Neads a title

Consider $$ L:V-V $$ , the dimension $$(v) = n$$ The matrix representation depends on an ordered Basis.

####Ex) 
>   $$ L:\mathbb{R}^2 \rightarrow \mathbb{R}^2  $$ 	Were  $$  L(x) = (x-x_1, 3x_2)^T L(e_1) = \left(\begin{array}{ccc} 1 \\ 0 \end{array} \right) $$ ,  $$ L(e_2) = \left[\begin{array}{ccc}-1\\3\end{array}\right] $$


>   The matrix representation with respect to $${\{e_1, e_2\}}$$ is $${\left[\begin{array}{ccc}1 & -1\\0&3\end{array}\right]}$$ 

>   Suppose we choose $$u_1 = \left(\begin{array}{ccc}1\\-1\end{array}\right), u_2 = \left(\begin{array}{ccc}-1\\2\end{array}\right)$$, $$  L(u_ 1) = Au_1 = \left(\begin{array}{ccc} 2 \\ -3 \end{array}\right), L(u_2) = Au_2 = \left(\begin{array}{ccc}-1\\6\end{array}\right)$$

>   We need $$L(u_1) = au_1+cu_2, L(u_2)=bu_1+du_2\implies\left[\begin{array}{ccc}a & b\\c & d\end{array}\right]$$ the transform from $$(u_1, u_2)\left[\begin{array}{ccc}1&1\\-1&2\end{array}\right]$$ the transform from $$ (e_1, e_2)\rightarrow[u_1, u_2]$$  is $$ U^{-1} = \left(\begin{array}{ccc}\frac{2}{3} & \frac{-1}{3}\\ \\\frac{1}{3} & \frac{1}{3}\end{array}\right)$$

>   $$  $$

>   $$ U^{-1} L(u_1) = U^{-1}+Au_1=\left(\begin{array}{ccc}\frac{7}{3}\\ \frac{-1}{3}\end{array}\right)$$ , $$ U^{-1} L(u_2) = U^{-1}+Au_2=\left(\begin{array}{ccc}\frac{8}{3}\\ \frac{5}{3} \end{array} \right)$$

>   $$  $$

>   $$L(u_1) = \frac{7}{3}v_1-\frac{1}{3}u_2$$ , $$L(u_2) = -\frac{8}{3}u_1 + \frac{5}{5}u_2$$

>   $$  $$

>   $$\Rrightarrow matrix \ B=\left(\begin{array}{ccc}\frac{7}{3}&\frac{8}{3}\\ \frac{1}{3}&\frac{5}{3}\end{array}\right)$$

$$  $$

The Columns of $$ B $$ are $$(U^{-1}Au_1,U^{-1}Au_2) = U^{-1}(Au_1,Au_2)$$ , $$ B = U^{-1}A U$$
	
###Definition:-
  If $$A$$ and $$B$$ are $$n\times n$$ matrices, and $$S$$ is not singular such that $$B = S^{-1}AS$$, then $$B$$ is similar to $$A$$
	
Since $$B = S^{-1}AS$$
$$SBS^{-1} = SS^{-1} A SS^{-1}\implies A = SBS^{-1} \Rightarrow A$$ is similar to $$B$$ 

So, if $$ B $$ is Similar to $$ A $$, $$ A $$ is Similar to $$ B $$

<!--BREAK-->

####Ex)  
>   Let $$D$$ be the differential operation on $$P_3$$ Find $$B$$ representing  $$D$$ W.R.T $$[1, x, x^2]$$ and $$A$$ representing $$D$$ W.R.T. $$[3, 4x, 2x^2+x]$$

>   Since

>	$$D(1) = 0 = 0 \cdot 1 + 0 \cdot x+ 0 \cdot x^2$$

>	$$D(x) = 1 = 1 \cdot 1+0 \cdot x + 0 \cdot x^2$$

>	$$D(x^2) = 2x = 0 \cdot 1+2 \cdot x + 0 \cdot x^2$$

>  $$B = \left(\begin{array}{ccc}0&1&0\\0&0&2\\0&0&0\end{array}\right)$$

>  $$  $$

>  Transition from $$[3, 4x, 2x^2=X]$$ to $$[1, x, x^2]$$ is, 
	
>   $$D(3)   = 0 =  0 \cdot 3 + 0 \cdot 4x + 0 \cdot (2x^2+x)$$

>   $$D(4x) = 4 = \frac{4}{3} \cdot 3 + 0 \cdot 4x + 0 \cdot (2x^2+x)$$

>   $$D(3)   = 4x+1 = \frac{1}{3} \cdot 3 + 0 \cdot 4x + 0 \cdot (2x^2+x)$$
	
>   $$ A = \left(\begin{array}{ccc} 0 & \frac{4}{3} & \frac{1}{3} \\ 0 & 0 & 1 \\ 0 & 0 & 0\end{array}\right)$$
	
>   $$  $$

	
>   To go from $$[3, 4x, 2x^2+x]$$ to $$[1, x, x^2]$$ , 
$$ S = \left(\begin{array}{ccc} 3 & 0 & 0 \\ 0 & 4 & 1 \\ 0 & 0 & 2 \end{array}\right)$$ ,  $$ S^{-1} = \left(\begin{array}{ccc} \frac{1}{3} & 0 & 0 \\ 0 & \frac{1}{4} & -\frac{1}{8} \\  0 & 0 & \frac{1}{2} \end{array}\right)$$ and $$S^{-1}BS=A$$

>   $$  $$

>   $$ A $$ is  a matrix representation W.R.T.  $$[1, x, x^2]$$

>   $$ B $$ is a matrix representation W.R.T. $$[3, 4x, 2x^2+x]$$
	
>   $$ S $$ is a transitional matrix from $$[3, 4x, 2x^2+x]$$ to $$[1, x, x^2]$$.

<!--BREAK-->

## Orthogonality in $$\mathbb{R}^2$$ and $$\mathbb{R}^n$$

2 vectors are orthogonal vectors at right angles ($$\mathbb{R}^2$$ and $$\mathbb{R}^3$$)

Suppose 
$$X = (x_1, x_2, \dots ,x_n)^T $$ , $$Y = (y_1, y_2, \dots , y_n)^T$$

From 1.5 the inner product $$X^TY$$ is 

$$X^TY = x_1y_1+x_2y_2+ \dots + x_n y_n = \sum\limits^{n}_{i=1}x_i y_i$$

in $$\mathbb{R}^2$$ and $$\mathbb{R}^3$$. We call this the Dot Product.
	
Eg)
	
>   $$X = \left(\begin{array}( 1 \\ 2 \\ 3 \end{array}\right)$$ ,  $$Y = \left(\begin{array}( 3 \\ 1 \\ -4 \end{array}\right)$$

>   $$  $$

>   $$X^TY = 1 \cdot 3 + (-2) \cdot 1 + 1 \cdot (-4) = -3$$
	
>   and
	
>   $$X = \left(\begin{array}( 1 \\ 2 \\ 3 \\ 4 \end{array}\right), Y=\left(\begin{array}( 4 \\ -1 \\ 1 \\ 2 \end{array}\right) \ X^TY = 1 \cdot 4 + 2 \cdot (-1) + 3 \cdot 1+ 4 \cdot 2 = 13$$


$$  $$


##Scalar Dot Product in $$\mathbb{R}^2$$ and $$\mathbb{R}^3$$
	
Since 

$$X^TX = (x_1, x_2)\left(\begin{array} {ccc}x_1\\x_2\end{array}\right) = x^2_1+x_2^2$$

$$  $$
	
$$||x||=\sqrt{X^TX}$$ in $$\mathbb{R}^3, ||X||=\sqrt{x_1^2+x_2^2+x_3^2}$$


For $$X,Y$$in $$\mathbb{R}^2$$ or $$\mathbb{R}^3$$ where $$\theta$$ is an angle between $$Z$$ and $$Y$$

$$  $$

$$  $$

<!--BREAK-->
	
##Law of Cosines $$X^T Y=||X|| ||Y||cos\left( \theta \right)$$
	
$$||X-Y||^2=||X||^2+||Y||^2-2||X|| ||Y||cos(\theta)$$

$$\implies ||X|| ||Y||cos(\theta)=\frac{1}{2}[||X||^2+||Y||^2-||X-Y||^2]$$

$$||X||||Y||cos(\theta) = \frac{1}{2}(X^TX+Y^TY-(X-Y)^T(X-Y))=\frac{1}{2}(X^TX+Y^TY-X^TX-Y^TY+X^TY+Y^TX)$$

$$=\frac{1}{2}(X^TY+Y^TX)$$

Since $$X^TY=Y^TX$$
$$=\frac{1}{2}(X^TY+X^TY)=\frac{1}{2}(2X^TY)=X^TY$$
	
A Unit vector in the $$X$$ direction $$U=\frac{X}{||X||}$$

$$||U|| = \left|\left|\frac{X}{||X||}\right|\right| = \frac{||X||}{||X||}=1$$

and

$$cos(\theta)=\frac{X}{||X||} = \frac{||X||}{||X||}=1$$

$$u=\frac{X}{||X||}, v=\frac{Y}{||Y||}, cos(\theta)=\frac{X^TY}{||X||||Y||}=\left(\frac{X}{||X||}\right)^T\left(\frac{Y}{||Y||}\right)^T=U^TV$$

$$  $$

$$  $$

## Cauchy - Schwartz Inequality
	
If X and Y are in $$\mathbb{R}^2$$ or $$\mathbb{R}^3$$ then 

$$|X^TY| \le\|X\| \|Y\|$$ 

with equality IFF $$x=0$$ or $$ y=0$$ or $$y=\alpha x$$
 
$$theta = \frac{\pi}{2}\Rrightarrow cos(\theta ) = \theta$$
	
2 vectors $$ X $$ and $$Y$$ $$\theta $$ $$\mathbb{R}^2$$ or $$\mathbb{R}^3$$ are called Orthogonal IFF $$X^TY = 0$$

EX)

>   $$\left(\begin{array}{ccc} 1 \\ 1 \end{array}\right) \ \left(\begin{array}{ccc} 1\\-1\end{array}\right) \implies (1, 1)^T \left(\begin{array}{ccc}1\\-1\end{array}\right)=1-1=0$$

## Scalar and vector projections let X and Y be in $$\mathbb{R}^2$$ and $$\mathbb{R}^3$$

$$\alpha = \|X\|cos(\theta) = \frac{X^TY}{\|Y\|} $$
	
is called Scalar Projection. 

### Projection vector 

$$P=\alpha\frac{Y}{\|Y\|} = \left(\frac{X^TY}{Y^TY}\right)y $$


$$  $$

$$  $$

## Orthoganalaty in $$\mathbb{R}^n$$

Let $$X $$ in $$\mathbb{R}^n$$ define Euclidian norm ( length ).
 
$$ \|X\|_2 = \sqrt{X^TX} = \sqrt{x_1^2 + x_2^2 + \dots + x_n^2}$$
	
We define the angle between 2 vectors $$X$$ and $$Y$$

$$cos(\theta) = \frac{X^TY}{\|X\|\|Y\|}$$ 

$$0\leq \theta \leq \pi$$
	
Define: 2 Vectors $$X $$ ,  $$Y $$ in $$ \mathbb{R}^n$$ are called orthogonal if $$X^TY = 0$$,
 
	
Notation is $$X \ \dot \ Y $$

####EX)

>   Find the point on $$Y = 3X$$, closest to the point $$(2, 1)$$

>   $$ V = \left(\begin{array}{ccc}2\\1\end{array}\right), \ \  W = \left(\begin{array}{ccc}1\\3\end{array}\right)$$

>   ( When $$ x $$ is 1 $$ y $$ is 3 )

>   $$Q = \left(\frac{Y^T W}{W^T W}\right) w = \frac{5}{10}\left(\begin{array}{ccc} 1 \\ 3 \end{array}\right)  = \left( \begin{array}{ccc}\frac{1}{2} \\ \frac{3}{2}\end{array}\right)$$


$$  $$
	
Ex) 
>   $$  $$Find the equation of a plane containing point $$(4, 1, -3)$$, perpendicular to the normal $$N=(1, 2, 4)  P_0 = (4, 1, 3),  P = (x, y, z)$$

>   $$\vec{P_0P}  = (x-4, y-1, 2-(-3))^T =(x-4, y-1, 2+3)$$

>   p in the plain 

>   $$\implies\vec{P_0P^T} N =0 \implies (x-4, y-1, 2+3)\left(\begin{array}{ccc}1\\2\\4\end{array}\right)=0\implies (x-4)+2(y-1)+4(2+3) = 0$$

$$  $$

In general a plain contains the point $$(x_0, y_0, z_0)$$ normal to $$N = (a, b, c)$$ is given by:

$$a(x-x_0)+b(y-t_0)+c(z-z_0) = 0$$
	
<!--BREAK-->
	
EX)
>    $$X=\left(\begin{array}{c} 1 \\ 2 \\ 1\\ -2 \end{array}\right), Y = \left(\begin{array}{c} 2 \\ 1 \\ 2\\ 1 \end{array}\right)$$

>    $$  $$

>    $$cos(\theta)=\frac{X^TY}{\|X\|\|Y\|} = \frac{2+2+2-2}{\sqrt{10} \sqrt{10}} = \frac{4}{10}=\frac{2}{3}$$

>    $$cos(\theta) = \frac{2}{3} \ \ \theta = acos\left(\frac{2}{3}\right)$$

Were $$0\le \theta \le \pi$$
	
$$  $$

$$  $$

<!--BREAK-->

#Lecture notes 2014/01/29

##Similar Matries

Consider $$L:V-v$$   $$ d $$ in $$ (v)=n $$

Matrix representation depends on an ordered basis

Ex)
>   $$  L: \mathbb{R}^2 \implies \mathbb{R}^2$$ were $$ L(x)=(x-x_1, 3x_2)^T $$

>   $$ L(e_1) = \left(\begin{array}{c} 1 \\ 1 \end{array}\right)$$ , $$ L(e_2)=\left(\begin{array}{c} -1 \\ 3 \end{array}\right) $$

>   The matrix representation with respect to $$ \{e_1, e_2\} $$ is $$ \left(\begin{array}{c}\ 1 & -1 \\ 0 & 3 \end{array}\right) $$

>   Suppose we choose $$ u_1 = \left(\begin{array}{c}1 \\ 0 \end{array}\right) $$ $$ u_2 = \left(\begin{array}{c}1 \\ 2 \end{array}\right) $$ 

>   $$ L(u_1) = Au_1 = \left(\begin{array}{c} 2 \\ -3 \end{array}\right) $$ , $$ L(u_1) = Au_1 = \left(\begin{array}{c} -1 \\ 6 \end{array}\right) $$ 

>   We need $$ L(u_1) = au_1 + cu_2 $$, $$ L(u_2) = bu_1 + du_2 \implies \left(\begin{array}{c} a & b \\ c & d \end{array}\right) $$ the transistion form
$$ [u_1, u_2] = [e_1, e_2] $$ if $$ (v_1  v_2) = \left( \begin{array}{c} 1 & 1 \\ -1 & 2 \end{array}\right)$$ the transform form $$ [e_1, e_2] \rightarrow [u_1, u_2] $$ is 
$$ U^{-1} = \left(\begin{array}{c} \frac{2}{3} & \frac{-1}{3} \\ \frac{1}{3} & \frac{1}{3}\end{array}\right) $$

>   $$  $$

>   $$ U_1^{-1}L(u_1) = U^{-1}Au_1 =  \left(\begin{array}{c} \frac{7}{3} \\ \frac{-1}{3}\end{array}\right) $$ , $$ U^{-1}L(u_2) = U^{-1}Au_2 = \left(\begin{array}{c} \frac{-8}{3} \\ \frac{5}{3}\end{array}\right) $$

>   $$  $$

>   $$ L(u_1) = \frac{7}{3}v_1 - \frac{1}{3}v_2 $$ , $$ L(u_2) = \frac{-8}{3}u_1 + \frac{5}{3}u_2$$

>   Matrix 

>   $$ B = \left(\begin{array}{c} \frac{7}{3} & \frac{-8}{3} \\ \frac{-1}{3} & \frac{5}{3}\end{array}\right) $$

>   $$  $$

The columns of B are $$ (U^{-1}Av_1 , \ U^{-1}Au_2) = U^{-1}(Au_1, \ Au_2) \ \therefore $$ 

$$B = U^{-1}AU$$

### Definition :-

*If $$ A $$ and $$ B $$ are two $$ n\times n $$, and $$ S $$ is  singular such that $$ B = SBS^{-1} $$ then $$ B $$ is similar to A Since $$ B = S^{-1}AS SBS^{-1} = SS^{-1}ASS^{-1} \Rightarrow A = SBS^{-1} \implies A$$ is similar to $$ B $$ So if $$ B $$ is similar to $$ A $$ and $$ A $$ is similar $$ B $$* 
 

EX) 
>   Let $$ D $$ be the differentiation  operation on $$ P_3 $$ find $$ B $$ representing $$ D $$ W.R.T. $$ [ \ 1, \ x, \ x^2 \ ] $$ and $$ A $$ represtnting $$ D $$ W.R.T. $$ [ \ 3, \ 4x, \ 2x^2 + x \ ] $$ Since 

>>   $$ D(1) = 0 = 0 * 1 + 0 * x + 0*x^2  $$

>>   $$ D(2) = 1 = 1*1 + 0 * x + 0 * x^2  $$

>>   $$ D(3) = 2x = 0*1 + 2 * x + 0 * x^2 $$

>   $$ B =  \left(\begin{array}{c} 0 & 1 & 0 \\ 0 & 0 & 2 \\ 0 & 0 & 0 \end{array}\right) $$

>   The transition from $$ [ \ 3, \ 4x, \ 2x^2 + x \ ] $$ to $$ [ \ 1, \ x, \ x^2 \ ] $$ is.

>>   $$ D(3) = 0 = 0*3 + 0*4 + 0*(2x^2 + x) $$

>>   $$ D(4x) = 4 = \frac{4}{3} + 0*4x + 0*(2x^2 + x) $$

>>   $$ D(2x^2 + x) = 4x + 1 = \frac{1}{3}*3 + 1*4x + 0(2x^2 +x) $$

>   $$ A = \left(\begin{array}{c} 0 & 0 & 0 \\ 0 & 4 & 1 \\ 0 & 0 & 2 \end{array}\right) $$ 

>   $$  $$

>   $$ B $$ is the matrix representation W.R.T. $$ [ \ 1, \ x \ x^2] $$ A is the matrix representation W.R.T. $$ [ \ 3, \ 4x, \ 2x^2 + x \ ] $$ $$ S $$ is the transition matrix from $$ [ \ 3,  \ 4x, \ 2x^2 + x \ ] $$ to $$ [ \ 1,  \ x, \ x^2 \ ] $$

$$  $$

$$  $$

<!--BREAK-->

#Class Notes 20140212

##Orthogonal Subspaces

Suppose $$ A $$ is $$ m \times n $$ and $$ x \ \epsilon  \ N(A) \implies Ax = 0$$ the $$ a_{i1}x_1 + 2_{i2}x_2 + \dots + a_{in}x_n = 0 \implies x $$ is orthogonal to the $$ i^{th} $$ of $$ A^T $$, so if $$ x $$ is orthogonal to the $$ i^{th} $$ columns of linear combinations of the columns of $$ A $$ we say $$ R(A^T) $$ and $$ N(A) $$ are orthogonal. 

###Definition :-

 *If $$ X $$ and $$ Y $$ are two subspaces of $$  \ \mathbb{R}^n $$ we say $$ X $$ is orthogonal to $$ Y $$ if and only if $$ \ X^TY= 0 \ \ \forall \ x \ \in \ X  $$ and $$ y \ \in \ Y $$*

$$  $$


####EX) 
>  Suppose $$ X $$ is spanned by $$ e_1 $$ and $$ Y $$ is spanned by $$ e_3 $$

>  $$ \implies \ x \ \in \ X $$  if and only if $$ x = \alpha \ \left(\begin{array}{c} 1 \\ 0 \\ 0 \end{array}\right) $$,
 $$  \ y \ \in \ Y  $$ if and only if $$ Y = \beta \ \left(\begin{array}{c} 0 \\ 0 \\ 1 \end{array}\right) $$,
 $$ X^TY = \alpha \ \left(\begin{array}\ 1 \ &  0 \ & 0\end{array}\right) \cdotp \ \beta \ $$ $$\left(\begin{array}{c} 0 \\ 0 \\ 1 \end{array}\right)  = \alpha \ \beta \ \left[\left(\begin{array}{c} \ 1 \ 0 \ 0 \end{array}\right) \left(\begin{array}{c} 0 \\ 0 \\ 1 \end{array}\right) \right]= 0$$

###Definition :-
 *Let $$ Y $$ be a subspace of $$ \mathbb{R}^n $$. The set of vectors in $$ \mathbb{R}^n $$ orthogonal  to $$ Y $$ are called the "Orthogonal" and "Compliment" of $$ Y $$, denoted $$ Y^{\perp} $$*


####EX)

>   Concider $$ \mathbb{R}^5 $$ were $$ X= span(e_1, e_2)) $$, $$ Y = span(e_3, e_4) $$ if $$ X \ \epsilon Y \ \implies\left(\alpha, \beta, 0, 0, 0 \right) $$ and $$ Y \ \epsilon Y \ \implies \left( 0, 0, \delta, \ \gamma, 0 \right) $$ $$ X^TY = 0 \implies X $$ and $$ Y $$ are orthogonal, but they are not Orthogonal Compiment                                                                                                                                                                                                     
  
>   $$ X^\perp = span(e_1, e_4, e_5) $$, $$ \frac{1}{Y} = span(e_1, e_2, e_5) $$
$$ \ne Y \ \ne X$$

$$  $$


## A Big Deal *Fundamental Subspaces*: 
Let $$ A $$ be an $$ m\times n $$ matrix range $$ R(A) = \{ b \in \mathbb{R}^n \ | \ b = Ax $$ for some $$ X \in \mathbb{R}^n \} = $$ Column space of $$ A \ (S \ \cdotp S \ \cdotp\mathbb{R}^m) $$

$$ R(A) = \{ y \in \mathbb{R}^n \ | \ Y=A^T $$ for some $$ X=\mathbb{R}^n \}$$

$$ S \ \cdotp S$$ of $$ \ \mathbb{R}^n $$

$$ N(A)=\{X\in \mathbb{R}^n \ | \ Ax = 0\} $$

$$ N(A^T)= \{Y \in \mathbb{R}^m \ | \ A^TY = 0\} $$

<!--BREAK-->

###Therom 5.2.1
Find the subspace theorem:- _If $$ A $$ is any $$ m\times n $$ Matrix_ 

$$ N(A)=(R(A^T))^\perp $$

$$ N(A^T) = (R(A))^\perp $$

Ex)
>   Let $$ A = \left(\begin{array}{c} 4 & 0 \\ 1 & 0 \end{array}\right) $$ and the Column space $$ \alpha \left(\begin{array}{c} 4 \\ 1\end{array}\right) + \beta\left(\begin{array}{c} 0 \\ 0 \end{array}\right) = \alpha \left(\begin{array}{c}4 \\1\end{array}\right)$$

>   $$ R(A)= \left\{ b \ | \ b = x_1 \left(\begin{array}{c}4 \\1\end{array}\right) \right\} $$ , 
>   $$ N(A^T)=\left\{ Y \ | \ A^TY = 0\right\} $$

>   $$ \left(\begin{array}{c} 4 & 1 \\ 0 & 0 \end{array}\right)\left(\begin{array}{c}Y_1 \\ y_2 \end{array}\right)= \left(\begin{array}{c}0 \\ 0 \end{array}\right)\Rightarrow Y = \beta \left(\begin{array}{c} 1 \\ 4 \end{array}\right)$$

>   $$ X^TY = \alpha \left(\begin{array}{c} 4 & 1 \end{array}\right) \cdotp \beta \left( \begin{array}{c} 1 \\ -4 \end{array} \right) = \alpha \ \beta \cdotp 0 = 0 $$
>   $$ \Rightarrow N(A^T)= \left[R(A)\right]^\perp $$ show true for $$ N(A) = \left[ R(A^T) \right]^ \perp $$

###Theorem 5.2.2
If $$ S $$ is a subspace of $$ \mathbb{R}^n $$ then $$ Dim(S) + Dim(S^\perp) = n $$. Also if $$ \{x_1, x_2, \dotsc,x_n\} $$ is a basis of $$ S $$ then $$ \{x_{n+1}, x_{n+2}, \dotsc , x_n \}  $$ is a basis for $$ S $$ then $$ \{x_1, x_2, \dotsc, x_n\} $$ is a basis for $$ \mathbb{R}^n $$

###Definition:-
The direct sum $$ W = U \oplus V: $$ given 2 subspaces $$ U $$ and $$ V $$ of $$ W $$ such that $$ W=U+V $$ for any $$ u=U $$ and $$ v=V $$

Ex)
>   Let $$ W=\mathbb{R}^3, u=span(e_1), v=span(e^2) $$ then $$ w \in W \iff W = u\oplus v $$

###Theorem 5.2.3
   $$ w=\alpha \ e_1 \ + \ \beta \ e_2 = span(e_2, \ e_2) $$ then $$ S $$ in a subspace of $$ \mathbb{R}^n $$ then $$ \mathbb{R}^n = S\oplus S^\perp $$

####ex)

>   $$ S = span (e_1), S^\perp = span(e_2, e_3) $$ then $$ S \oplus S^\perp = span(e_1, e_2, e_3) = \mathbb{R}^3 $$

###Theorem 5.2.4
If $$ S $$ is  subspace of $$ \mathbb{R}^n $$ then $$ (S^\perp) = S $$

<!--BREAK-->

#### Ex)
>   Let $$ A = \left(\begin{array}{c} 1 & 2 & 1 \\ 0 & 1 & 2 \\ 1 & 1 & -1 \end{array}\right) $$ find a basis for $$ N(A), R(A^T), N(A^T), R(A) $$. To find $$ N(A) $$ and $$ R(A^T) $$ we have to row reduce $$ A $$

>   $$  $$

>   $$ \implies  \left(\begin{array}{c} 1 & 0 & -3 \\ 0 & 1 & 2 \\ 0 & 0 & 0 \end{array}\right) $$ 

>   $$  $$
   
>   $$N(A)\implies x_1 = 3x_3, \ x_2 = -2x_3 $$

>   $$ x \in N(A) \iff X = \alpha \left(\begin{array}{c} 3 \\ -2 \\ 1 \end{array}\right) , \ R(A^T)$$ is spaned by $$ \left(\begin{array}{c}1 \\ 0 \\ -3\end{array}\right)\cdotp\left(\begin{array}{c} 0 \\ 1 \\ 2 \end{array}\right) $$

>   $$  $$

>   $$Y\in R(A^T)Y = \beta \ \left(\begin{array}{c} 1 \\ 0 \\ -3 \end{array}\right) + \delta \ \left(\begin{array}{c} 0 \\ 1 \\ 2 \end{array}\right)  $$

>   $$ X^TY = 0 $$ 
>>   $$ Dim(N(A)) = 1 $$

>>   $$ Dim(R(A^T)) = 2 $$

>>   $$ Dim(N(A)) + Dim(R(A^T)) = 3 $$ 

>   $$ A^T = \left(\begin{array}{c} 1 & 0 & 1 \\ 2 & 1 & 1 \\ 1 & 2 & -1 \end{array}\right) \rightarrow \left(\begin{array}{c} 1 & 0 & 1 \\ 0 & 1 & -1 \\ 0 & 0 & 0 \end{array}\right) $$ 

>   $$  $$

>>   So 
   
>   $$ X \in R(A) = \alpha \left(\begin{array}{c}1 \\ 0 \\ 1 \end{array}\right) + \beta \ \left(\begin{array}{c} 0 \\ 1 \\ -1 \end{array} \right) $$

>   $$   $$

>   $$ Y \in N(A^T) = \delta \ \left(\begin{array}{c} -1 \\ 1 \\ 1 \end{array}\right) $$

>   $$  $$

>   $$ X^TY =0 \implies N\left(A^T\right) = \left[R(A)\right]^\perp$$


<!--BREAK-->

# Class notes 20140226
## Orthoganal Projectoions

Let $$ W $$ be a subspace of $$ \mathbb{R}^n $$ and $$ dim W = k, $$ $$ W = span\{x_1, x_2, \dots,x_k\} $$
 $$  $$  $$ \hat{v} \in W $$closest to $$ v $$

![Orthogonal Projection](file:///Users/JimW/Desktop/OrthProject.tif)

Answer: Want $$ \hat{v} \in W $$ so that $$ v-\hat{v} \in W^\perp $$ 

$$ \hat{v}=  $$ is the Orthoganal projection of $$ V $$ into $$ W $$ We need $$ \hat{v} $$ so that 

$$ V-\hat{v} \perp x_1 \implies (V-\hat{v}) \cdot x_1 = 0 \\ V-\hat{v} \perp x_2 \implies (V-\hat{v}) \cdot x_2 = 0 $$ 

$$   \vdots  $$ 

$$ V-\hat{v} \perp x_n \implies (V-\hat{v}) \cdot X_n = 0$$

$$ X = \pmatrix{ x_1  \cr x_2 \cr \vdots \cr x_n} Y =  \pmatrix{ y_1  \cr y_2 \cr \vdots \cr y_n}  \ \ \ \ X \cdot Y = X^T Y \ \ \ = \pmatrix{ x_1  \ x_2  \cdots  x_n} \pmatrix{ y_1  \cr y_2 \cr \vdots \cr y_n} = \sum_{i=1}^{n} X_n\cdot Y_n$$

$$ X_1^T(V-v^n)=0 \implies x_1^TV-X_1^T\hat{v}=0\implies X_1^TV=X_1^\hat{v}$$ 

$$ \vdots $$

$$ X_k^T(V-v^n)=0 \implies x_k^TV-X_k^T\hat{v}=0\implies X_k^TV=X_k^\hat{v} $$

$$ \pmatrix{ x_1^T  \cr x_2^T \cr \vdots \cr x_k^T } \pmatrix{ \cr \cr V \cr \cr }=\pmatrix{ x_1^T  \cr x_2^T \cr \vdots \cr x_k^T } \pmatrix{ \cr \cr \hat{v} \cr \cr }$$

Let $$ A=\pmatrix{ X_1 & | &  X_2 & | & \cdots & | & X_k } $$ $$  $$ $$ A^TV=A^T\hat{v} $$

col Space $$ (A)=W $$ $$ A^T $$ is $$ n \times k $$ and now invertable $$ A $$ is a $$ n\times k $$ Matrix $$ \hat{v} \in W $$ So $$ \hat{v}=c_1x_1+c_2x_2+\cdots c_k x_k $$

$$ c_1, c_2, \cdots , c_k \in \mathbb{R} $$

$$ \hat{v}= \pmatrix{ X_1 & | &  X_2 & | & \cdots & | & X_k } \pmatrix{ c_1 \cr c_2 \cr \vdots \cr c_k }= AC \therefore A^T = A^T\hat{v}=A^TAC $$

$$ A^TA $$ is $$ n\times k $$ and $$ k \times n \implies A^TA\rightarrow k \times k $$ and square, so invertible.

###Prop
Since $$ x_1, \cdots , x_k $$ are independent, $$ A^TA $$ has an inversion.  

$$ A^TV=A^TAC \implies (A^TA)^{-1}A^TV=C$$

###Formula For Projection

$$ W=span\{x_1,\cdots , x_k\} $$ $$ X_1, \cdots x_k $$ are independent. $$ V\in \mathbb{R} $$ the projection of $$ v $$ onto $$ W=\hat{V}=A(A^TA)^{-1}A^TV $$

$$ Q_w = $$ Projectio matrix onto $$ W $$

[URL for Lectures](faculty.uml.edu/dklain/projections.pdf)

If $$ \pmatrix{ A \cr \vdots \cr A^n  }\pmatrix{ x \cr y } = (?) $$ an over determined system

If $$ Ax=d $$ has no solutions, what's the best approximation for a solution? If $$ x_1A_1+\cdots x_nA_n $$ can't solve $$ Ax=b $$, try $$ A^TAv=A^Tb $$

set $$ x=(A^TA)^{-1}A^Tb $$ The least squares approximation 

>####e.g.
Compute the projection matrix $$ Q $$ for the 2D subspace $$ W $$ of $$ \mathbb{R}^4 $$ spanned by $$ (1, 1, 0, 2) $$ and $$ (-1, 0, 0, 1) $$
What id the orthogonal projection of $$ (0, 2, 5, -1) $$ onto $$ W $$

>> $$  $$$$ A = \pmatrix{ 1 & -1 \cr 1 & 0 \cr 0 & 0 \cr 2 &1 }$$

>> $$  $$$$ A^TA = \pmatrix{ 1 & 1 & 0 & 2 \cr -1 & 0 & 0 & 1 }\pmatrix{1 & -1 \cr 1 & 0 \cr 0 & 0 \cr 2 &1 } = \pmatrix{ 6 & 1 \cr 1 & 2 } $$

>> $$  $$$$ (A^TA)^{-1} = \frac{1}{det(A)} = \frac{1}{12-1}\pmatrix{2 & -1 \cr -1 & 6} = \pmatrix{ \frac{2}{11} & \frac{-1}{11} \cr \frac{-1}{11} & \frac{6}{11} }  = \frac{1}{11}\pmatrix{ 2 & -1 \cr 1 & 6 }$$

>> $$ Q = \frac{1}{11}\pmatrix{ 1 & -1 \cr 1 & 0 \cr 0 & 0 \cr 2 & 1}\pmatrix{ 2 & -1 \cr -1 & 6 }\pmatrix{ 1 & 1 &0 & 2 \cr -1 & 0 & 0 & 1 } $$ 
>> $$ =\frac{1}{11}\pmatrix{ 1 & -1 \cr 1 & 0 \cr 0 & 0 \cr 2 & 1 } \pmatrix{ 3 & 2 & 0 & 3 \cr -7 & 1 & 0 & 4 } = \frac{1}{11}\pmatrix{ 10 & 3 & 0 & -1 \cr 3 & 2 & 0 & 3 \cr 0 & 0 & 0 &0 \cr -1 & 3 & 0 & 10 }$$ 

> $$  $$

>> b) $$ Q\pmatrix{ 0  \cr 2 \cr 5 \cr -1 } = \frac{1}{11}\pmatrix{ 7 \cr 1 \cr 0 \cr -4 } $$

$$ Q_wQ_w = A\overbrace{(A^TA)^{-1}A^TA}^{I}(A^TA)^{-1}A^T \implies Q_w^2A(A^TA)^{-1}A^T = Q_w$$

It is Indempotant. 

###Line fit with least squares 

Fitting a line to data as the best fit we can. 

![Scatter Plot](file:///Users/JimW/Documents/UMassLowell/classes/LinearAlgebra/scaterplot.tiff) Wishful thinking, we need $$ y = ax+b $$ as a best fit

$$ \matrix{ y_1 = mx_1 + b  \\ y_2 = mx_2 + b \\ \vdots \\ y_n= mx_n + b } \implies \pmatrix{ y_1 \\ y_2 \\ \vdots \\ y_3} = \pmatrix{ x_1 & 1 \cr x_2 & 1 \cr \vdots & \vdots \\ x_n & 1 }\pmatrix{ m \\ b } $$

$$ Y=AC $$ Has no solution so multiply both sides by $$ A^T $$ giving $$ A^TY = A^TAC \implies (A^TA)^{-1}A^TY = C = \pmatrix{ m \\ b } $$ we have

$$ A^TA = \pmatrix{ x_1 & \cdots & x_n \cr 1 & \cdots & 1 }\pmatrix{ x_1 & 1 \cr \vdots & \vdots \cr x_n & 1 }=\pmatrix{ \sum x_i^2 & \sum x_i \cr \sum x_i & n } $$

####E.X.

> We have a data set $$ (1,3),(2,2),(3,0),(4,-1),(5,-3) $$ find the Best fit line. 

>![Least Squares fit](file:///Users/JimW/Documents/UMassLowell/classes/LinearAlgebra/leastSqEg.tiff)
>$$ \pmatrix{ 3 \\ 2 \\ 0 \\ -1 \\ -3 } \matrix{ ? \\ = \\ No!}\pmatrix{ 1 & 2 \cr 2 & 1 \\ 3 & 1 \\ 4 & 1 \\ 5 & 1 }\pmatrix{ w \\ b }\longrightarrow $$ Least Squares. 

> $$ \pmatrix{ m \\ b } \approx (A^TA)^{-1}A^TY$$

> $$ A^TA = \pmatrix{5^2+4^2+3^2+2^2+1^2 & 5+4+3+2+1 \\ 5+4+3+2+1 & 5} = \pmatrix{55 & 15 \\ 15 & 5} $$

> $$ (A^TA)^{-1} = \frac{1}{ 5(55)-15^2 } \pmatrix{ 5 & -15 \\ -15 & 55 }  = \frac{1}{50} \pmatrix{5 & -15 \\ -15 & 55} = \pmatrix{ \frac{1}{10} & \frac{-3}{10} \\  \frac{-3}{10} & \frac{11}{10} } $$

> $$ \pmatrix{ \frac{1}{10} & \frac{-3}{10} \cr \frac{-3}{10} & \frac{11}{10} }\pmatrix{ 1 & 2 &3 & 4 & 5\cr 1 & 1 & 1 & 1 & 1}\pmatrix{ 3 \\ 2 \\ 0 \\ -1 \\ -3 } = \pmatrix{ 0.1 & -0.3 \cr -0.3 & 1.1 } \pmatrix{ -12 \\ 1 } = \pmatrix{ -1.2 - 0.3 \cr 3.6 + 1.1 }  = \pmatrix{ -1.5 \\ 4.7 } $$

> $$ \therefore \matrix{ m = -1.5 \\ b = 4.7 }  \implies y \approx -1.5x + 4.7 $$


####In polynomial form. 
$$ y=ax^2+bx+c $$ we need

$$ \pmatrix{y_1 \\ y_2 \\ \vdots\\ y_n } = \pmatrix{ a_1x_1^2+b_1x_1+c  \cr a_2x_2^2+b_2x_2+c \cr \vdots \cr a_nx_n^2+b_nx_n+c } \implies \pmatrix{ y_1 \\ y_2 \\ \vdots\\ y_n } = \pmatrix{ x_1^2 & x_1 & 1 \\ x_2^2 & x_2 & 1 \\ \vdots & \vdots & \vdots \\ x_n^2 & x_n & 1} \pmatrix{ a \\ b \\ c }$$

$$ Y = AC \implies A^TY=A^TAC \implies (A^TA)^{-1}A^TY = C $$

e.x
>  $$ x_1 + x_2 = 3, \ \ 2x_1 - 3x_2 = 1, \ \ 0x_1 + 0x_2 = 2 $$ 

>  $$ \pmatrix{1 & 1 \\ 2 & -3 \\ 0 & 0} \pmatrix{ x_1 \\ x_2 } = \pmatrix{ 3 \\ 1 \\ 2 }, \ \ \ A = \pmatrix{ 1 & 1 \cr 2 & -3 \\ 0 & 0}$$ 

>  $$ \pmatrix{ 1 & 2 & 0 \\ 1 & -3 & 0} \pmatrix{ 1 & 1 \cr 2 & -3 \\ 0 & 0 } \pmatrix{ x_1 \\ x_2 }  = \pmatrix{ 1 & 2 & 0 \\ 1 & -3 & 0} \pmatrix{ 3 \\ 1 \\ 2 }$$

>  $$ \pmatrix{ 5 & -5 \cr -5 & 10 } \pmatrix{ x_1 \\ x_2 } = \pmatrix { 5 \\ 0 } \implies \pmatrix{ 1 & -1 \\ 1 & -2 } \pmatrix{ x_1 \\ x_2 } = \pmatrix{ 1 \\ 0 } $$

>  $$ \pmatrix{ x_1 \\ x_2 } = \pmatrix{1 & 1 \\ -1 & -2} \pmatrix{ 1 \\ 0 }$$

>  $$ \pmatrix{ -2 & 1 \\ -1 & 1 } \pmatrix{ 1 \\ 0 } = \pmatrix{ 2 \\ 1 } = \hat{X}$$

e.x.

>  $$ P = A \hat{X} = \pmatrix{ 1 & 1 \cr 2 & -3 \\ 0 & 0} \pmatrix{ 2 \\ 1 } = \pmatrix{3 \\ 1 \\ 0} $$

>  $$ r(\hat{X}) = b - p  = \pmatrix{ 3 \\ 2 \\ 1 } - \pmatrix{ 3 \\ 1 \\ 0} = \pmatrix{ 0 \\ 0 \\ 2} $$

>  $$ A^Tr = \pmatrix{ 1 & 1 & 0 \\ 1 & -3 & 0 \\ } \pmatrix{ 0 \\ 0 \\ 2 } = \pmatrix{ 0 \\ 0 }$$

>  $$ \implies r \in N(A^T) $$



e.x.

>  $$ A = \pmatrix{ 1 & 2 \\ 2 & 4 \\ -1 & -2 }, \ \ \pmatrix{ 3 \\ 2 \\ 1 } $$ Solve $$ A^TA\hat{x} = A^Tb $$

>  $$ \pmatrix{ 1 & 2 & -1 \\ 2 & 4 & -2 } \pmatrix{ 1 & 2 \\ 2 & 4 \\ -1 & -2 } \pmatrix{ x_1 \\ x_2 } = \pmatrix{ 1 & 2 & -1 \\ 2 & 4 & -2 } \pmatrix{ 3 \\ 2 \\ 1 } $$

>  $$ \pmatrix{ 1 & 2 \\ 1 & 2 } \pmatrix{ x_1 \\ x_2 } = \pmatrix{ 1 \\ 1 } $$

>  $$ x_2 = \alpha, x_1 = 1-2x_2 = 1-2\alpha $$

>  $$ \hat{x} \pmatrix{ 1-2\alpha \\ \alpha } = \pmatrix{ 1 \\ 0 } + \alpha \pmatrix{ -2 \\ 1 }$$ has $$ \infty \ \# $$ of solutions

e.x.

>  $$ \pmatrix{2-2\alpha \\ 1- \alpha \\ \alpha } = \hat{x} $$ 



> x  | -1| 0 | 1 | 2
> ---|---|---|---|--
> y  | 0 | 1 | 3 | 9 

>  $$ \pmatrix{1 & x_1 \\ \vdots & \vdots \\ 1 & x_n } \pmatrix{c_0 \\ c_1 } = \pmatrix{ y_1 \\ \vdots \\ y_n }  \ \ \  y = C_0 + C_1 x$$

>  $$ \pmatrix{ 1 & -1 \\ 1 & 0 \\ 1 & 1 \\ 1 & 2 } \pmatrix{ c_0 \\ c_1 } = \pmatrix{ 0 \\ 1 \\ 3 \\ 9 } $$

>  $$ \pmatrix{ 1 & 1 & 1 & 1 \\ 1 & 0 & 1 & 2 } \pmatrix{ 1 & -1 \\ 1 & 0 \\ 1 & 1 \\ 1 & 2 } \pmatrix{ c_0 \\ c_1 } = \pmatrix{ 1 & 1 & 1 & 1 \\ -1 & 0 & 1 & 2}\pmatrix{ 0 \\ 1 \\ 3 \\ 9 } $$

>  $$  \pmatrix{ 4 & 2 \\ 2 & 6 } \pmatrix{ c_0 \\ c_1 }= \pmatrix{ 13 \\ 21 } $$

>  $$  \pmatrix{c_0 \\ c_1} = \pmatrix{ 4 & 2 \cr 2 & 6 }^{-1} \pmatrix{ 13 \\ 21 } $$

>  $$ = \frac{1}{20}\pmatrix{ 6 & -2 \cr -2 & 4 }\pmatrix{ 13 \\ 21 } = ? = \pmatrix{ \frac{35}{20}  \\ \frac{38}{20} } $$

>  $$ y = C_0 + C_1 x = \pmatrix{ \frac{9}{5} \\ \frac{29}{10} } $$

>  $$ y = \frac{9}{5} + \frac{29}{10}x $$

If $$ (x_1,y_1)\dots (x_n, y_n) $$ and $$ x=( \ x_1 \ \ x_2 \ \ \dots \ \ x_n \ ) $$ and $$ y = ( \ y_1 \ \ y_2 \ \ \dots \ \ y_n \ ) $$. 
Then $$ \bar{x} = \frac{1}{n} \sum_{i=1}^{n} x_i $$ and $$ \bar{y} = \frac{1}{n} \sum_{i=1}^{n} y_i $$

Let $$ y = C_0 + C_1 x $$ be a liner function of best fit, show that if $$ \bar{x}=0, C_0 = \bar{y}, C_1 = \frac{X^TY}{X^TX} $$ 

$$ \pmatrix{ 1 & x_1 \\ 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n} \pmatrix{ c_0  \cr c_1  } = \pmatrix{y_1 \\ y_2 \\ \vdots \\ y_n } $$

$$ \pmatrix{ 1 & 1 & \dots & 1 \\ x_1 & x_2 & \dots &x_n } \pmatrix{ 1 & x_1 \cr 1 & x_2 \\ \vdots & \vdots \\ 1 & x_n } \pmatrix{ c_0  \\ c_1  } = \pmatrix{ 1 & 1 & \dots & 1 \\ x_1 & x_2 & \dots & x_n } \pmatrix{ y_1 \\ y_2 \\ \vdots  \\ y_n }$$

$$ \pmatrix{ n & \sum_{i=1}^{n} x_i \cr \sum_{i=1}^{n} x_i & \sum_{i=1}^{n} x^2_n } \pmatrix{ c_0  \\ c_1 } = \pmatrix{ \sum_{i=1}^{n} y_1 \\ \sum_{i=1}^{n}x_iy_i }$$

$$ c_0 = \frac{1}{n}\sum_{i-1}^{n}y_1 = \bar{y}, \ \ c_1 = \frac{\sum_{i=1}^{n} x_i \ y_1}{\sum_{i=1}^{n} x_i \ x_i} = \frac{X^TY}{X^TX}$$

>  $$ P=A(A^TA)^{-1}A^T $$  for $$ A $$ is an $$ m \times n $$ rank $$ n $$ Show $$ P^2  = P$$ prove $$ P^k = P $$.

>  $$ P^2 = [A \ (A^TA)^{-1} \ A^T][A \ (A^TA)^{-1} \ A^T] $$

>  $$ = A \ (A^TA)^{-1} \ A^TA(A^TA)^{-1} \ A^T $$

>  $$  = A\ (A^TA)^{-1}A^T = P $$

>  Prove $$ P^k = P $$

>  Assume $$ P^n = P $$ show true for $$ m + 1 $$

>  $$ P \ P^m = P \cdot P $$

>  $$ P^{m + 1} = P^2 = P $$

> Show $$  P  $$ is $$ sym \implies P^T = P $$

>  $$ (A(A^TA)^{-1} \ A^T) = (A^T)^T [ A^TA]^{-1} \ A^T = A[A^T A^{T^T}]^{-1}A^T  = A[A^TA]^{-1}A^T = P$$

> $$  \implies P^T = P \implies P $$ is Symmetrical

## 5.4 Inner Product Space

Def:- 
An inner product on a vector space $$ V $$ is an operation that assosiates a real number $$ \langle X,Y \rangle $$ to each pair of vectors $$ X,Y \in V $$ such that 

1.   $$ \langle X , Y \rangle = \langle Y, X \rangle$$
2.   $$ \langle X , X, \rangle \ge 0 $$ with equality if and only if $$ x=0 $$
3.   $$ \langle \alpha X + \beta Y , 2 \rangle = \alpha\langle X,2\rangle + \beta \langle Y, 2 \rangle$$


####ex) 
1.   $$ \mathbb{R}^n \ \langle X, y \rangle = X^TY $$
2. Weighted inner product
3.   $$ \langle X, Y \rangle = \sum_{i=1}^{n} w_i $$

####1a)

>  $$ A=\pmatrix{ 3 & 4 \cr 6 & 8 }\rightarrow\pmatrix{ 3 & 4 \cr 0 & 0 }\implies R(A^t) $$ has a basis $$ \pmatrix{ 3  \cr 4  } $$

>  $$ \pmatrix{ 3 & 4 \cr 0 & 0 } \pmatrix{ x_1  \cr x_2  } = \pmatrix{ 0  \cr 0  } \implies \pmatrix{ x_1  \cr x_2  } = \pmatrix{ 4  \cr -3  }$$ 

>  $$ N(A) $$ has the basis $$ \pmatrix{ 4  \cr -3  } \ \ R(A) $$ has the basis $$ \pmatrix{ 2  \cr -1  } $$
 
> $$ N(a) $$$$ \pmatrix{ 1 & 2 \cr 0 & 0 } \pmatrix{ x_1  \cr x_2  } = \pmatrix{ 0  \cr 0  } \implies \pmatrix{ x_1  \cr x-2  } = \pmatrix{ 2  \cr -1  } $$

>  The Basis for $$ N(A^T) $$ is $$ \pmatrix{ 2  \cr -1  } $$ or $$ \pmatrix{ -2  \cr 1  } $$

####1c)

>  $$ A=\pmatrix{ 4 & -2 \cr 1 & 3 \cr 2 & 1 \cr 3 & 4 } \rightarrow \pmatrix{ 1 & 0 \cr 0 & 1 \cr 0 & 0 \cr 0 & 0 } $$ Basis for $$ R(A) $$ is $$ \pmatrix{ 1  \cr 0  } \ \pmatrix{ 0  \cr 1  } $$ 

>  $$ N(A) \ \ \ \pmatrix{ 1 & 0 \cr 0 & 1 \cr 0& 0 \cr 0 & 0 } \pmatrix{ x_1  \cr x_2  } = \pmatrix{ 0  \cr 0  } \implies \pmatrix{ x_1  \cr x_2  } = \pmatrix{ 0  \cr 0  }$$


####ex)

>  $$ e[a,b] $$

>  $$ \langle \ f, g \ \rangle = \int_{a}^{b} f \cdot g \ dx  $$ or weighted $$ \langle \ f, g \ \rangle = \int_{a}^{b} w \ f \cdot g \ dx $$ where $$ w( \ x \ )> 0 $$

Define length or norm. $$ \|v\| = \sqrt{\langle v, v, \rangle} $$

1.   $$ \langle v, v \rangle = \sum v_i^2 \ \ \ \ \|v\| = \sqrt{\sum v_i^2}$$
2.   $$ u \perp v \iff \langle u, v\rangle = 0 $$


####Thm : Pythagorean Law

$$ u \perp v \ \ \ \| u + v \|^2 = \| u \|^2 + \| v \|^2$$

$$ \| u + v \|^2 = \langle u + v , u + v \rangle = \langle u, u \rangle + \overbrace{\langle v, u \rangle + \langle u , v \rangle}^{=0} + \langle v, v \rangle = \| u \|^2 + \| v \|^2 $$


####ex)

>   $$ e[-1, 1] $$

>   $$ u = 1 \ , \ \ v = x^2 \ \ \ \ \langle 1 , x^2 \rangle = \int_{-1}^{1} 1 \cdot x^2 \ dx \ = \ 0  $$

>   length $$ \| 1 \|^2 = \langle 1, 1 \langle = \int_{-1}^{1} 1 \cdot 1 \ dx = 2  $$

>   $$ \| x^3 \| ^2 = \langle x^3 , x^3 \rangle = \int_{-1}^{1} x^3 \cdot x^2 \ dx = \frac{2}{7}  $$

>   So $$ \| 1 \| = \sqrt{2} \ \ \ \ \| x^3 \| = \sqrt{\frac{2}{7}} $$

>   One can say $$ \| 1 + x^2 \| = \frac{16}{7} = 2 + \frac{2}{7} $$


#### For $$ \mathbb{R}^{m\times n} $$ Frobeio's Norm.

$$ \| \cdot \|_f $$ where 

$$ \| A \|_f = \langle A, A \rangle^{\frac{1}{2}} = \pmatrix{ \sum_{i=1}^{n} \sum_{j=1}^{n} a_{i j}^{2} }^{\frac{1}{2}} $$

####e.g. 
$$ A = \pmatrix{ 1 & 2 & 4 \cr -1 & 3 & 2 } \ \ \ \|A\|_f = \pmatrix{ 1+4+16+1+9+4}^{\frac{1}{2}} = \sqrt{35}$$ 

Consider $$ P_n(x) $$ polynomials of degree $$ < \ n $$. Let $$ {x_1, \dots , x_n} $$ be $$ n $$ distinct numbers. Let $$ p(x), \ q(x) \in P_n(x) $$

$$ \langle p, q \rangle = \sum_{i=1}^{n} p(x_i)q(x_i) $$ 

is also the inner product. 

1.   $$ \langle P, p \rangle = \sum_{i=1}^{n} P^2 (x_i) \ge 0$$ 

2.   Since $$ \sum_{i=1}^{n} p(x)q(x_i) = \sum_{i=1}^{n} Something $$

3.   $$ \langle \alpha p + \beta q, n \rangle = \sum_{i=1}^{n} \lgroup \ \alpha \ p(x_i) + \beta \ p \ (x_i) \ \rgroup \ r(x_i) = \sum_{i=1}^{n} \matrix{ \alpha \ p(x_i) & \dots \cr \vdots } $$ 

#### e.x.

>   $$ x_i = \frac{i-1}{2}, \ \ i = 1, 2, 3, \ \ \ \ \ x_1 = 0, \ \ x_2 = \frac{1}{2}, \ \ x_3 = 1 $$

>   $$ \| 2x \| = \sqrt{\langle 2x, 2x \rangle} = \lgroup \sum_{i=1}^{n} 4x^2 \rgroup ^{\frac{1}{2}} = (0 + 1 + 4 )^{\frac{1}{2}} = \sqrt{5}$$


####Def:

If $$ u_1 v_1 \in V $$ and $$ \sqrt{ } \ne 0 $$ Then the scalar projection of $$ u $$ onto $$ v $$ is $$ \alpha = \frac{\langle u, v \rangle}{\| v \|} $$ the vector projection 

$$ P = \alpha \pmatrix{ \frac{v}{\| v \|}} = \pmatrix{ \frac{\langle u,v \rangle}{\| v \|} }^{\frac{1}{2}}$$

#### The Couchy Schwartz Inequality

let $$ u $$ and $$ v $$ be the inner product space $$ V $$, 
then $$ \lvert \langle u, v \rangle \rvert \le \| u \| \| v \|$$ with equality $$ \iff u = \alpha \ v $$ 
i.e. $$ u $$ and $$ V $$ are linearly independent. 

#### Proof

If $$ v \equiv 0 \ \ \ \ \lvert \langle u, v \rangle \rvert = \| u \| \cdot 0 = 0$$

Let $$ P $$ be the vector projection of $$ u $$ onto $$ v $$, since $$ P $$ and $$ U \cdot P $$ are orthogonal

$$ \| u - p \|^2 + \|p \|^2 = \| u \|^2$$

so

$$ \| p \|^2 = \| u \|^2 - \| u - p \|^2 = \bigg\lbrack \cfrac{\lvert \langle u, v \rangle \rvert }{\| V \|^2} \cdotp \| v \|^2 \bigg\rbrack $$

$$ \implies \lvert \langle u, v \rangle \rvert = \| v \|^2 \pmatrix{ \| u \|^2 - \| u - p \|^2 } \le \|v\|^2\|u\|^2 $$

$$ \implies \lvert \langle U, V \rangle \rvert \le \| u \| \| v \| $$

equality $$ \iff \| u - p \|^2  = 0 \implies u = p $$ either $$ v=0  $$ or $$ u $$ and $$ v $$ are in the same direction since.

$$ \lvert \langle u, v \rangle \rvert \le \|u\|\|v\| \le \langle u, v \rangle \le \|u\|\|v\|-1 \le \cfrac{\langle u, v \rangle}{\|u\|\|v\|} = \cos(\theta) $$


#### Def:-
Normalled linear vector space given a vector space $$  V  $$ if each $$ v\in V $$ has a real number .............

1.   $$ \|v\| \ge 0 \ \ \ \| v \| = 0 \iff v = 0 $$

2.   $$ \| \alpha \| = | \alpha | \|v\| \Delta $$ inequality. 

3.   $$ \| u + v \| \le \| u \| + \| v \| $$


#### Theorem. 
If $$ V  $$ is an inner ptroduct space, then 

$$  \|v\| = \langle v, v \rangle ^{\frac{1}{2}} \ \ \forall \ \ v \in V $$ 

defines a norm on $$ V $$

1.   $$ \langle v, v \rangle^\frac{1}{2} \ge 0  $$ SInce $$ \langle v, v, \rangle \ge 0 $$

2.   $$ \| \alpha v \| = \langle \alpha \ v,  \alpha \ v \rangle ^ \frac{1}{2} = \lbrack\alpha^2 \langle v, v \rangle \rbrack ^\frac{1}{2} = \pmatrix{ \alpha ^2 } ^\frac{1}{2} \ \langle v, v \rangle ^\frac{1}{2} = \lvert \ \alpha \ \rvert \langle v, v \rangle^\frac{1}{2} = \lvert \ \alpha \ \rvert \| v \|$$

3.   $$ \| u + v \|^2 = \langle u + v, u + v \rangle = \langle u, u \rangle + 2 \langle u, v \rangle + \langle v, v \rangle \le \| u \|^2 + 2 \| u \| \| v \| + \| v \| ^2 \le \pmatrix{ \| u \| + \| v \| }^2 $$

$$ \| u + v \| \le \| u \| \| v \| $$ 

Some other norms $$ V\equiv \mathbb{R}^n $$

1.   $$ \| x \|_1 = \sum_{i=1}^{n} \lvert x_1 \rvert $$

2.   $$ \|x\|_\infty = \underset{i}{\max} \lvert x_i \rvert $$

3.   P-norm $$ \| x\|_p = \bigg\lbrack \sum_{i=1}^{n} \lvert x_i \rvert ^P \bigg\rbrack ^ \frac{1}{P} $$ for $$ P = 2 \implies $$ Euclidian Norm. $$ \iff P \ne P_0 $$ Theorem won't hold. 

#### Def:-
If $$  x, y \in V $$ where $$ V $$ is a N.L.V.S, then $$  \| x - y \| \equiv $$ to the distance between $$ x $$ and $$ y $$

####e.x
 
>    Suppose $$  \langle f, g \rangle = \int_{-1}^{1} f \cdot g \ dx  \  $$  define  $$ \| \cdot \| = \langle f, g \rangle^\frac{1}{2}$$

>   if $$ f = x  $$ and $$  g = x^2 $$

>   $$ \| \ f - g \ \| = \| \ x - x^2 \ \| =  \bigg\lbrack \int_{-1}^{1} (x-x^2)(x-x^2) dx \bigg\rbrack ^\frac{1}{2}  $$ = distance between functions

## 5.5 Ortho Normal Spaces (sets)

#### Def:-
Let $$ \lbrace v_1, v_2, \dots , v_n  \rbrace  $$ be non zero vectors in an inner product space $$ V $$ if $$  \langle v_i, v_i \rangle = 0 $$ whenever $$  i \ne i $$ then the set is said to be orthogonal

####e.g.

>   $$ \pmatrix{ 1 \cr 2 \cr 1  }, \pmatrix{ 2 \cr -1 \cr 0 } ,\pmatrix{ 2 \cr 4 \cr -10 } $$ is an orthogonal set. 

#### Thm:-
If $$ \lbrace v_1,  \dots , v_n \rbrace $$ is an orthogonal set of vectors in an inner product $$  V $$, then $$ \lbrace v_1,  \dots , v_n \rbrace $$ is linearly independent. From

$$ c_1 v_1 + c_2 v_2 + \dots +c_nv_n = 0 $$

$$ \langle c_1v_1 + c_2v_2 + \dots c_nv_n, v_i \rangle = 0 $$

$$ c_1 \langle v_1, v_i \rangle + c_2 \langle v_2, v_i \rangle + \dots + c_n \langle v_n, v_i \rangle = 0$$

$$ \implies c_i \langle v_j, v_i \rangle = 0$$

$$ c_i \| v_i \|^2 = 0 $$

$$ \implies c_i = 0 $$

#### Def:-
An orthogonal set of vectors is an Orthogonal set of unit vectors. The set $$ \lbrace u_1, u_2, \dots , u_n \rbrace  $$ is orthogonal is $$  \langle u_i, u_i \rangle = \delta_ij = \cases{1, & i = j \cr 0, & i ≠ j}$$    (Kronken Delta Function)


#### Def:-
Suppose $$ B = \lbrace u_1, u_2, \dots, u_n \rbrace $$ is an orthonormal basis for $$ S $$, then $$  S = span(u-1, \dots , u_n) $$

####Perseval's Formula. 

if $$ \lbrace u_1, u_2, \cdots , u_n\rbrace $$ is an orthonormal basis for an inner product space $$ V $$ and $$ V=\sum_{i=1}^{n} c_iu_i $$ then $$  \| u \|^2 = \sum c_i^2 $$


####e.g.
>   $$ u_1 = \pmatrix{\frac{1}{\sqrt{5}} \ , \ \frac{2}{\sqrt{5}}} \ , \ u_2 = \pmatrix{\frac{-2}{\sqrt{5}} \ , \ \frac{1}{\sqrt{5}}} $$ 

>   if $$ X\in \mathbb{R}^2 $$ then $$ X^Tu_1 = \cfrac{x_1 + 2x_2}{\sqrt{5}} $$    and   $$ X^Tu_2 = \cfrac{-2x_1 + x_2}{\sqrt{5}} $$

>   $$ \| x \| ^2 =  \pmatrix{\cfrac{x_1 + 2x_2}{\sqrt{5}}} + \pmatrix{\cfrac{-2x_1 + x_2}{\sqrt{5}}} = x_1^2 + x_2^2 $$


### Orthogonal Matrices

#### Def:-
An $$ n \times n $$ matrix $$ Q $$ is called orthogonal if the Column vectors of $$ Q $$ form an orthogonal set in $$ \Rn $$ 

#### Thm:-
 An $$ n\times n $$ matrix $$ Q $$ is orthogonal $$ \iff Q^TQ\cdot I = Q^{-1} = Q^T $$ 

$$ Q =  \rotateDD  $$

$$ \bigg\| \pmatrix{\cos(\theta) \\ \sin(\theta) } \bigg\| = \cos^2(\theta)+\sin^2(\theta) = 1$$

$$ Q^T = \pmatrix{ \cos(\theta) & \sin(\theta) \cr -\sin(\theta) & \cos(\theta) } $$

$$ Q^TQ = \pmatrix{ \cos(\theta) & \sin(\theta) \cr -\sin(\theta) & \cos(\theta)} \rotateDD = $$

$$\pmatrix{ \cos^2(\theta)+\sin^2(\theta) & -\cos(\theta)\sin(\theta)+\cos(\theta)\sin(\theta) \cr -\sin(\theta)\cos(\theta)+\sin(\theta)\cos(\theta) & \sin^2(\theta)+\cos^2(\theta) } = \pmatrix{1 & 0 \cr 0 & 1} = I$$ 

If $$ Q $$
 is an $$ n\times n $$ orthogonal matrix, 

1. The Column vectors are orthogonal in $$ \Rn $$

2.    $$ Q^TQ = I $$

3.    $$ Q^{-1}  = Q^T$$

4.    $$ \langle  \ Q_x \ , \ Q_y \ \rangle = \langle  \ x \ , \ y \ \rangle $$ 

5.    $$ \| Q_x \|_2 = \| x \|_2 $$


## Least Squares. 

Solve $$ Ax = b  $$ for $$ A: m \times n $$ then $$ A^TA\hat{x} = A^Tb $$

### Thm:-
If the columns of $$ A $$ form an orthoganal set on $$ \Rn $$ then $$ A^TA = I $$ and $$ \hat{x} = A^Tb $$

#### e.x.

Find the best least squares approximation to $$ \cos (x) $$ on $$ [0, 1] $$ by a linear function. 

Let $$ S $$ be the subspace of all linear functions in $$ C[0, 1] $$, then $$ u_1 = 1 \ , \ u_2 = \sqrt{12} \ ( \ x \ - \ \frac{1}{2}) $$ is an orthogonal set of vectors. 

Let 

$$ c_1 = \int_{0}^{1} u_1 \cos(x) \ dx = 1 \\ c_2 = \int_{0}^{1} u_2 \cos(x) \ dx = \sqrt{12} \ \int_{0}^{1} \bigg(x - \cfrac{1}{2} \bigg) cos(x) \ dx $$

$$ = \bigg( \cfrac{1}{2} \ \sin(1)  + \cos(-1)  \ \bigg)x + \bigg(\cfrac{3}{4}\sin(1) - \cfrac{1}{2}\cos(1) + \cfrac{1}{2} \bigg)$$



# Class Notes 03/26/2014

Let $$ L(x) = (x_2 + x_3, x_1+x_3, x_1+x_2)^T $$ Determin the standard matrix. 

$$ L(e_1)= L\pmatrix{ 1 \\ 0 \\ 0 } = \pmatrix{0 \\ 1 \\ 1 } $$ 

$$ L(e_2)= L\pmatrix{ 0 \\ 1 \\ 0 } = \pmatrix{1 \\ 0 \\ 1 } $$  

$$ L(e_3)= L\pmatrix{ 0 \\ 0 \\ 1 } = \pmatrix{1 \\ 1 \\ 0 } \\  $$

$$ A = \pmatrix{ 0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0} $$

$$ L\pmatrix{1 \\ 3 \\ 2} = A \pmatrix{ 1 \\ 3 \\ 2 } = \pmatrix{ 0 & 1 & 1 \\ 1 & 0 & 1 \\ 1 & 1 & 0}\pmatrix{ 1 \\ 3 \\ 2 }= \pmatrix{ 5 \\ 3 \\ 4 }$$


Let $$ A $$ and $$ B $$ be similar matrices, $$ \lambda $$ is any scalar. Show $$ A - \lambda I $$ and $$ B - \lambda I $$ are similar.

$$ A = S^{-1}BS $$

$$ A-\lambda I = S^{-1}BS - \lambda I $$

$$ A-\lambda I = S^{-1}BS -S^{-1}A(\lambda I) = S^{-1}BS - S^{-1}(\lambda I)S $$

$$ A-\lambda I = S^{-1}(B-\lambda I)S \implies A-\lambda I $$ and $$ B-\lambda I $$ are similar



 

Let $$ A $$ be $$ n \times n $$, if there exists a nonzero vector $$ x $$, so that $$ Ax = \lambda x $$ for some scalar $$ \lambda $$, then we call $$ \lambda $$ an Eigenvalue of $$ A $$ and $$ x $$ is the associated Eigenvector (Not Unique)

#### e.x.
>   $$  A = \pmatrix{3 & -3 \\ -4 & 2 } \ , \ x = \pmatrix{-2 \\ 2} $$

>   $$ Ax = \pmatrix{3 & -3 \\ -4 & 2 }\pmatrix{-2 \\ 2} = \pmatrix{ -12 \\ 12 } = 6\pmatrix{-2 \\ 2 } = 6 x $$

>   So $$ Ax = 6x \ , \ \lambda=6 \ , \ x = \pmatrix{ -2 \\ 2 } $$ 


If $$ A $$ is $$ n\times n $$ and $$ \lambda $$ is an eigenvalue with eigenvectors $$ x $$

a. $$ \lambda $$ is an eigenvalue of a. 

b. $$ (A-\lambda I)x $$ has nontrivial solutions. 

c.   $$ N(A-\lambda ) \ne 0 $$.

d. $$ A-\lambda I  $$ is singular. 

e.   $$  \det(A-\lambda I) = 0 $$.

The last part $$  \det(A-\lambda I) \equiv n^{th} $$ order polynomial in $$ \lambda $$ is called the Characteristic Ploynomial.

#### e.g.
FInd the eigenvalues and eigenvectors for $$ A = \pmatrix{ 1 & 5 \\ 2 & -2 } $$

Form $$ (A- \lambda I) = \pmatrix{ 1-\lambda & 5 \\ 2 & -2-\lambda } $$ 

We want $$ \det (A-\lambda I) = \bigg \lvert \matrix{ 1-\lambda & 5 \cr 2 & -2-\lambda } \bigg \rvert  = 0$$

$$ (1-\lambda)(-2-\lambda) - 10 = 0 $$

$$ \lambda^2 + \lambda - 12 = 0 \implies (\lambda +4)(\lambda -3) = 0 $$

$$ \lambda = -4 \ , \ \lambda = 3 $$
to get the eigenvectors $$  x_1 , x_2 $$ solve $$ (A-\lambda I)x = 0 $$ 

$$ \lambda_1 = -4 \ , \ \bigg \lvert \matrix{ 1-(-4)  & 5 \cr 2 & -2-(-4) } \bigg \rvert  = \pmatrix{ 5 & 5 \cr 2 & 2 } x_1 = \pmatrix{ 0 \\ 0 } \implies x_1 \alpha \pmatrix{ 1 \\ -1 }$$

$$ \lambda_2 = 3 \ , \ \bigg \lvert \matrix{ 1-3  & 5 \cr 2 & -2-3 } \bigg \rvert  = \pmatrix{ -2 & 5 \cr 2 & -5 } x_2 = \pmatrix{ 0 \\ 0 } \implies x_2 \alpha \pmatrix{ 5 \\ 2 }$$

eigenspace has basis vectors $$  \pmatrix{ 1 \\ -1} \ , \ \pmatrix{ 5 \\ 2 } $$

$$ v \in  $$ eigenspace of $$ A \iff v = c_1 \pmatrix{ 1 \\ -1} + c_2 \pmatrix{ 5 \\ 2 } $$

In General

$$ A= \pmatrix{ a & b \cr c & d } $$

$$ A - \lambda I =  \bigg \lvert \matrix{ a-\lambda  & b \cr c & d-\lambda } \bigg \rvert = (a-\lambda)(d-\lambda) - bc = 0 $$ 

$$\implies \lambda^2 - \mathrm{tr}(A)\lambda + \det(A) = 0 $$

#### Thm:-
If $$ A $$ and $$ B $$ are similar, they have the same characteristic polynomials. $$ \implies $$ same eigenvalues. ( eigenvectors will be diffwerent, possibly )

### Complex Eigenvalues. 

$$ Ax = \lambda x $$ and $$ A $$ is Real and $$ \lambda  $$ is complex,

$$ \bar{Ax} = \bar{\lambda x} $$

$$ A\bar{x} = \bar{\lambda} \bar{x} $$

$$ \implies \bar{x} $$ is a eigenvector assosciated with $$ \bar{\lambda} $$

$$ \lambda = a-b\bar{\mathbb{i}} \ , \ \bar{\lambda} = a-b\mathbb{i} $$

$$ \lambda = 1 - 3\mathbb{i} \ , \ \bar{\lambda} = 1 +3\mathbb{i} $$

### Systems of constant coefficient. linear differential equations. 

In 1-D $$ y\prime = 3y $$ is a 1^st order differential equation. 

$$ y  \ \alpha  \ \mathbb{e}^{3t} \ , \  y = c\mathbb{e}^{3t}  $$

$$ y \prime = c(3\mathbb{e}^{3t}) = 3(c\mathbb{e}^{3t})  = 3y$$

In general 

$$ y \prime = a y $$ 


assume

$$ y(t)=\mathbb{e}^{\lambda t} $$

$$ \lambda \mathbb{e}^{\lambda t} \implies \lambda = a $$

The solution is 

$$ y = c \mathbb{e}^{a t} $$

#### First order systems

$$ \pmatrix{ \matrix{y_1\prime & = & a_{11}y_1 & + & a_{12}y_2 & + & \dots & + & a_{1n} y_n } \\ \matrix{ y_2\prime & = & a_{21}y_1 & + & a_{22}y_2 & + & \dots & + & a_{2n} y_n } \\ \matrix{\vdots &  \ &  \  & \vdots & \ & \ & \ & \vdots & \ & \ & \ & \ & \ & \vdots } \\ \matrix{y_n\prime & = & a_{n1}y_1 & + & a_{n2}y_2 & + & \dots & + & a_{nn} y_n } }$$

In part 2-D 

$$ y_1\prime = a y_1 + b y_2 \\ y_2\prime = c y_1 + d y_2$$

$$ A = \pmatrix{ a_{\imath \jmath} } $$

$$ Y = \pmatrix{y_1, y_2, \dots y_n}^T $$

$$ Y\prime = \pmatrix{y_1\prime, y_2\prime, \dots y_n\prime}^T $$

$$ Y\prime = A Y $$ looks like 1-D $$ y\prime = ay $$ Look for solution to 

$$ Y = \pmatrix{x_1 \mathbb{e}^{\lambda t} \cr x_2\mathbb{e}^{\lambda t} \cr \vdots \cr x_n\mathbb{e}^{\lambda t}} = \mathbb{e}^{\lambda t}x$$

Where $$ x= \pmatrix{x_1, x_2, \dots , x_n}^T $$ if $$ Y = \mathbb{e}^{\lambda t} = \lambda \mathbb{e}^{\lambda t} x $$

$$ \mathbb{e}^{\lambda t} \ne 0 $$ devide by it $$ Ax = \lambda x $$  standard eigenvalue problem

In General we solve 

$$ Y\prime = AY $$

for initial condition $$ Y(0)=Y $$

#### e.g.
$$ y_1 \prime = 2y_1 + 2y_2 \ , \ y_1(0)=3$$

$$ y_2 \prime = 2y_1 + 5y_2 \ , \ y_2(0)=1$$

So

$$ Y^\prime = \pmatrix{2 & 2 \cr 2 & 5}Y $$

for $$ Y = \pmatrix{ y_1 \\ y_2} $$ and $$ A = \pmatrix{2 & 2 \\ 2 & 5} $$

The Characteristic polynomial is

$$ \lambda^2 - 7 \lambda + 6 = 0 $$

$$ (\lambda - 6)( \lambda -1) = 0 $$

$$ \lambda_1 = 1  $$ Solve $$ (A - \lambda I)x = 0 $$

$$ \pmatrix{ 1 & 2 \cr 2 & 4 }x_1 = \pmatrix{ 0 \\ 0} \ , \ x_1 \ \alpha \pmatrix{ 2 \\ -1 } $$

$$ \lambda_2 = 6 $$ Solve $$ (A - \lambda I)x = 0 $$

$$ \pmatrix{ -4 & 2 \cr 2 & -1 }x_2 = \pmatrix{ 0 \\ 0} \ , \ x_2 \ \alpha \pmatrix{ 1 \\ -2 }$$

The solution is 

$$ Y = c_1 \pmatrix{ 2 \\ -1 } \mathbb{e}^{t} + c_2 \pmatrix{ 1 \\ 2 } \mathbb{e}^{6t} $$

$$ t = 0 \ , \ Y(0) = \pmatrix{ 2c_1 + c_2 \\ -c_1 + 2c_2 } = \pmatrix{ 3 \\ 1 }   \implies c_1 = c_2 = 1$$

$$ Y(t) = \pmatrix{ y_1(t) \\ y_2(t) } = \pmatrix{ 2 \\ -1 } \mathbb{e}^t + \pmatrix{ 1 \\ 2 } \mathbb{e}^{6t}$$


#### // insert a picture of tank mixing problem. 

> Tank A initially has 40g of salt, 

> Tank B initially has 100g of salt.

> Find the amount of salt in either tank at time $$ (t) $$

> Let $$ y_1(t) =  $$ the amount of salt in tank A

> and $$ y_2(t) =  $$ the amount of salt in tank B

> Concentration in A is $$ \dfrac{y_1(t)}{100} $$, B is $$ \dfrac{y_2(t)}{500} $$

> $$ \dfrac{dy_1}{dt} \equiv $$ of the rate of change of the salt in tank A

> $$ \dfrac{dy_2}{dt} \equiv $$ of the rate of change of the salt in tank B

> $$ \dfrac{dy_1}{dt} = - \bigg(\dfrac{y_1}{100} \bigg)(30) + \bigg(\dfrac{y_2}{500}\bigg)(10) $$

> $$ \dfrac{dy_2}{dt} =  \bigg(\dfrac{y_1}{100} \bigg)(30) - \bigg(\dfrac{y_2}{500}\bigg)(10) - \bigg (\dfrac{y_2}{500} \bigg )(20) $$

> $$ \dfrac{dy_2}{dt} = - \bigg(\dfrac{y_1}{100} \bigg)(30) - \bigg(\dfrac{y_2}{500}\bigg)(30) $$

> $$ Y_1(0) = 40g \ , \ y_2(0) = 100g $$

> Define $$ Y = \pmatrix{ y_1(t) \\ y_2(t)} $$

> $$ Y^\prime = \pmatrix{-\frac{3}{1} y_1 - \frac{1}{50} y_2 \\ \frac{3}{10} y_1 - \frac{3}{50} y_2 }$$ 

> $$ Y^\prime = AY \ , \ A = \pmatrix{-\frac{3}{10} & \frac{1}{50} \\ \frac{3}{10} & -\frac{3}{50}} $$

> $$ Y_0 = \pmatrix{ 40 \\ 100 }$$

> Assume $$ Y = x\mathbb{e}^{\lambda t} $$

> $$ \lambda^2 + \frac{18}{50}\lambda - \frac{6}{500} = 0 $$

> Charcteristic Polynomial is 

> $$ \lambda^2 + \frac{9}{25}\lambda - \frac{3}{250} = 0 \implies \lambda_1 \ , \ \lambda_2$$ 

# Lecture notes 2014/04/02

## sec 5.5 homework solutions. 

1. a)  $$ \pmatrix{ 1 \cr 0 } \pmatrix{ 0 \\ 1 } \ , \ \pmatrix{ 1 \\ 0}^T \pmatrix{ 0 \\ 1 } = \pmatrix{ 1 & 0 } \pmatrix{ 0 \\ 1 }$$

>    $$ \bigg \vert \bigg \vert \pmatrix{ 1 \\ 0 } \bigg \vert \bigg \vert = \sqrt{1^2 + 0^2}$$

>    $$ \bigg \vert \bigg \vert \pmatrix{ 1 \\ 0 } \bigg \vert \bigg \vert = 1 $$

>  b)  $$ \pmatrix{ \frac{3}{5} \\ \frac{4}{5} } \pmatrix{ \frac{5}{13} \\ \frac{12}{13}} $$ These are a unit. But $$ \pmatrix{ \frac{3}{5} & \frac{4}{5} } \pmatrix{ \frac{5}{13} \\ \frac{12}{13}} \ne 0$$ So not orthogonal

>  c) $$ \pmatrix{ 1 \\ -1 } \pmatrix{ 1 \\ 1} $$ are not unit vectors, they are orthogonal.

>  d) $$ \pmatrix{ \frac{\sqrt{2}}{2} \\ \frac{1}{2}} \pmatrix{ \frac{1}{2} \\ \frac{\sqrt{2}}{2}}  $$

>  $$ \pmatrix{ \frac{\sqrt{2}}{2} & \frac{1}{2} } \pmatrix{ \frac{1}{2} \\ \frac{\sqrt{2}}{2}} = - \frac{\sqrt{3}}{4}+ \frac{sqrt{3}}{4} = 0 \implies $$ are orthoganal. 

>  $$ \bigg \vert \bigg \vert \pmatrix{\frac{\sqrt{2}}{2} \\ \frac{1}{2}} \bigg \vert \bigg \vert = \sqrt{\pmatrix{\frac{\sqrt{3}}{2}}^2 + \pmatrix{\frac{1}{2}}^2} = \sqrt{\frac{3}{4}+ \frac{1}{4}} = 1 \implies $$ Unit vectors 

Given a subspace spanned by orthogonal vectors, the projection matrix is $$ UU^T $$ were the collumns of $$ U $$ are.

$$ u_2 = \pmatrix{\frac{2}{3} \\ \frac{2}{3} \\ \frac{1}{3}} \ , \ u_3 = \pmatrix{ \frac{1}{\sqrt{2}} \\ -\frac{1}{\sqrt{2}} \\ 0}$$

$$ U = \pmatrix{u_1 & u_2} = \pmatrix{\frac{2}{3} & \frac{1}{\sqrt{2}} \\ \frac{2}{3} & -\frac{1}{\sqrt{2}} \\ \frac{1}{3} & 0 } $$

$$ P = UU^T = \pmatrix{ \frac{17}{18} & -\frac{1}{18} & \frac{2}{9} \\ -\frac{1}{18} & \frac{17}{18} & \frac{2}{9} \\ \frac{2}{9} & \frac{2}{9} & \frac{1}{9}} $$

$$ x = \pmatrix{ 1 \\ 2 \\ 3} $$

projection of $$ x $$ onto the subspace by $$ u_2 $$ and $$ u_3 $$ is,

$$ p = Px = \pmatrix{\frac{23}{18} \\ \frac{41}{18} \\ \frac{8}{9}} $$

$$ p-x=\pmatrix{\frac{5}{18} \\ \frac{5}{18} \\ -\frac{10}{9}} $$

$$ (p-x)^Tu_2  = \pmatrix{\frac{5}{27} + \frac{5}{27} - \frac{10}{9} } = 0$$

$$ (p-x)^Tu_3 = \pmatrix{\frac{5}{18\sqrt{2}} - \frac{5}{18\sqrt{2}} } = 0 $$  

1) 

> c)

>   $$ x_1 = \pmatrix{\cos \theta \\ \sin \theta } \ , \ x_2 = \pmatrix{-\sin \theta \\ \cos \theta}$$

>   $$ \bigg \vert \bigg \vert x_1 \bigg \vert \bigg \vert = \sqrt{ \cos^2 \theta + \sin^2 \theta } \  , \ \bigg \vert \bigg \vert x_2 \bigg \vert \bigg \vert = \sqrt{\sin^2 \theta + \cos^2 \theta }$$

>   $$ X_1^T X_2 = - \cos \theta \sin \theta +\sin \theta \cos \theta = 0 $$

> b) 

>    $$ Y = c_1 x_1 + c_2 x_2 $$

>    $$ c_1 = \langle y \ , \ x_1 \rangle \ , \ c_2 = \langle y \ , \ x_2 \rangle \implies $$ $$ c_1 = y_1 \cos \theta + y_2 \sin \theta \\ c_2 = - y_1 \sin \theta + y_2 \sin \theta $$

>   $$ c_1^2 + c_2^2 = \vert \vert Y \vert \vert = y_1^2 + y_2^2 $$ 

>   $$= ( y_1^2 \cos^2 \theta + 2 y_1 y_2 \cos \theta \sin  \theta + y_2^2 \sin^2 \theta ) + (y_1^2 \sin^2 \theta - 2 y_1 y_2 \sin \theta \cos  \theta + y_2^2 \cos^2 \theta) $$

>   $$ = y_1^2 (\cos^2 \theta + \sin^2 \theta) + 0 + y_2^2(\sin^2 \theta + \cos^2 \theta) $$

>   $$ = y_1^2 + y_2^2 $$

7)

>   $$ x = c_1 u_1 + c_2 u_2 +c_3 u_3 $$

>   $$ \vert \vert x \vert \vert = 5 \ , \ \langle u_1 \ , \ x \rangle = 4 \ , \ x \perp u_2$$

>   $$ c_1 = \langle x \ , \ u_1 \rangle \ , \ c_2 = \langle x \ , \ u_2 \rangle \ , \ c_3 = \langle x \ , \ u_3 \rangle $$

>   $$ c_1^2 + c_2^2 +c_3^2 = \vert \vert x \vert \vert ^2 = 25$$

>   $$ c_1 = 4 \ , \ x \perp u_2 \implies c_2 = 0 $$

>   $$ 16 + 0 + c_3^2  = 25 \implies c_3^2 = 9 \implies c_3 = \pm 3 $$

####13)
 Show $$ (Q^m)^{-1} = (Q^T)^m = (Q^m)^T  \ , \ Q$$ is $$ n\times n $$ orthogonal matrix.

>   a) $$ m = 1 \ , \ Q^{-1} = Q^T = Q^T $$ true for $$ Q $$ othogonal 

>   $$ m=2 \ , \ (Q^2)^{-1} = (Q \cdot Q )^{-1} = Q^{-1} \cdot Q^{-1} = Q^T Q^T = (Q^T)^2 $$

>   Assume true for $$ m = k $$

>   $$ (Q^k)^{-1} = (Q^T)^k $$

>   Show true for all $$ m = k + 1 $$

>   $$ (Q^{k+1})^{-1} = (Q^k Q)^{-1} = Q^{-1}(Q^k)^{-1} = Q^T(Q^T)^k = (Q^T)^{k-1}$$

   $$ e[-1 \ , \ 1] \ , ] \langle f \ , \ g \rangle = \int_{-1}^{1} f \cdot g \ dx  $$

Show $$ 1 $$ and $$ x $$ are orthogonal

>   $$ \langle 1 \ , \ x \rangle = \int_{-1}^{1} 1 \cdot x \ dx  = \int_{-1}^{1} x \ dx = \cfrac{x^2}{2} \bigg\vert_{-1}^1 = \cfrac{1}{2} - \cfrac{1}{2} = 0$$

   $$ \vert \vert 1 \vert \vert = \sqrt{ \langle 1 \ , \ 1 \rangle} $$

>  $$ = \sqrt{\int_{-1}^{1} dx } = \sqrt{2} $$

   $$ \vert \vert x \vert \vert = \sqrt{ \langle x \ , \ x \rangle} $$

>  $$ \sqrt{\int_{-1}^{1} x^2 \ dx } = \sqrt{\cfrac{2}{3}} = \cfrac{\sqrt6}{3} $$

## Higher order systems. 

Look at $$ Y^\prime = AY \ , \ Y(0) = Y_0$$ where $$ A $$ was $$ 2 \times 2 $$

$$ Y = x\mathbb{e}^{\lambda t} \implies Ax = \lambda x$$


>  $$ y^{\prime \prime } + 3y^\prime - 4y = 0 $$

>  $$ y^{\prime \prime} = 4y - 3y^\prime $$

>> Let $$ y = y_1 $$

>>  $$ y^\prime = y_2 = y_1^\prime $$

>>  $$ y^{\prime \prime} = y_2^\prime $$

>  $$ Y = \pmatrix{y_1 \\ y_2} \implies Y^\prime  = \pmatrix{ 0 & 1 \cr 4 & -3 }Y$$

>  $$ A = \pmatrix{ 0 & 1 \cr 4 & -3 } \ , \ \bigg\vert  \ A - \lambda I \ \bigg\vert = \lambda^2 + 3 \lambda -4 = 0 \implies \lambda_1 = 1 \ , \ \lambda_2 = -4$$

>  $$ \lambda_1 = 1 $$ Solve $$ (A-I)x_1 =0$$

>>  $$ \pmatrix{ -1 & 1 \cr 4 & -4 }x_1 = 0 \implies x_1 = \pmatrix{ 1 \\ 1 } $$

>  $$ \lambda_2 = -4 $$ Solve $$ ( A + 4I )x_2 = 0 $$

>>  $$ \pmatrix{ 4 & 1 \cr 4 & 1 }x_2 = 0 \implies x_2 = \pmatrix{ 1 \\ -4 }$$ 

> So $$ Y = c_1\pmatrix{ 1 \\ 1 }e^t + c_2\pmatrix{ 1 \\ -4 } e^{-4t} $$

>  $$ y = y_1 = c_1 e^t + c_2 e^{-4t} \ , \ Y(0) = \pmatrix{ 2 , 7 } $$ where $$ y(0) = 2 \ , \ y^\prime (0) = 7 $$

>  $$ \implies y = 3e^t - e^{-4t} $$

### In General. 
$$ Y^{\prime \prime} = A_1Y + A_2 Y^\prime $$

can be translated into a 1^st order problem

$$ \matrix{ y_{n+1} (t) = y_1^\prime \\ y_{n+2} (t) = y_2^\prime \\ \vdots \\ y_{2n}(t) = y_n^\prime} $$

Let

$$ Y_1 = Y = ( y_1, \dots ,y_n)^T $$

$$ Y_2 = Y_1^\prime = ( y_1^\prime , \dots , y_n^\prime)^T$$

$$ \implies $$ 
$$Y_1^\prime = 0 + Y^2 \\ Y_2^\prime = A_1 Y_1 + A_2 Y_2 $$

$$  \pmatrix{Y_1^\prime \\ y_2^\prime} = \pmatrix{ 0 & I \cr A_1 & A_2 } \pmatrix{Y_1 \\ Y_2} $$


#### e.g.

>   $$ y_1^{\prime \prime } = -2y_2 $$

>   $$ y_2^{\prime \prime } = y_1 + 3y_2 $$

>>  Let

>>   $$ (y_3 = y_1^\prime) \ , \ y_3^\prime = y_1^{\prime \prime} = -2y_2 $$

>>   $$ (y_4 = y_2^\prime) \ , \ y_4^\prime = y_1 + 3y_2$$

>>   $$ \implies \pmatrix{y_1 \\ y_2 \\ y_3 \\ y_4 } \implies Y^\prime = AY$$ for $$ A = \pmatrix{ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & -2 & 0 & 0 \\ 1 & 3 & 0 & 0 } $$

>   $$ \bigg\vert A - \lambda I \bigg\vert = \lambda^4 - 3\lambda^2 + 2 = 0 \implies \lambda^2 = 1 $$ or $$ \lambda^2 = 2 $$

>   $$ \lambda_1 = 1 \space \lambda_2 = -1 \space \lambda_3 = \sqrt{2} \space \ \lambda_4 = -\sqrt{2} $$

>   $$ x_1 = \pmatrix{ -2 \\ 1 \\ -2 \\ 1 } \ , \space x_2 = \pmatrix{-2 \\ 1 \\ 2 \\ -1} \ , \space x_3 = \pmatrix{ 1 \\ -1 \\ \sqrt{2} \\ -\sqrt{2} } \ , \ x_4 = \pmatrix{ 1 \\ -1 \\ -\sqrt{2} \\ \sqrt{2} } $$

>   $$ Y(t) = c_1 x_1 e^t + c_2 x_2 e^{-t} + c_3 x_3 e^{\sqrt{2}t} + c_4 x_4 e^{-\sqrt{2}t}$$

## 6.3 Diagonalization

### Thm:-

If $$ \lambda _1 , \dots , \lambda_n $$ are distinct eigenvalues of an $$ n \times n $$ matrix, with corresponding eigenvectors $$ x_1, \ x_2, \ \dots , \ x_n $$ , then the eigenvectors are Linearly Independent.

### Defiition:-

An $$ n \times n  $$ matrix $$ A $$ is diagonolizable $$ \iff $$ there exsists a nonsingular matrix $$ X $$ and a diagonal matrix $$ D $$ such that

$$ X^{-1} A X = D $$

We say that $$ X $$ Diagonalizes $$ A $$

### Thm:-

An $$ n \times n $$ matrix $$ A $$ is diagonizable $$ \iff \ A$$ has $$ n $$ Linear Independent eigenvectors.

1. $$ X $$ has collumns that are eigenvectors of $$ A $$ and $$ D = diag(\lambda_1 \lambda_2, \dots , \lambda_n) $$
2. $$ X $$ is not unique (reorder eigenvalues)
3. If $$ A $$ is diagonal, then $$ A = X D X^{-1} $$

Note :- If $$ A = X D X^{-1} \implies A^2 = (X D X^{-1})(X D X^{-1}) = X D^2 X^{-1}$$

In general, 

$$ A^k = X D^k X^{-1} $$

#### e.g.

>   $$ A = \pmatrix{ 5 & 6 \cr -2 & -2 } \ , \ \bigg\vert A - \lambda I \bigg\vert  = (5 - \lambda)(-2 - \lambda) + 12 = 0$$

>   $$ \implies \lambda^2 -3 \lambda +2 = 0 \implies (\lambda -2)(\lambda -1) = 0 $$

>   $$ \lambda-1 = 1 \ , \ \lambda_2 = 2 $$

>   $$ x_1 = \pmatrix{ 3 \\ -2} \ , \ x_2 = \pmatrix{ -2 \\ 1 }$$

>   $$ X = \pmatrix{ 3 & -2 \cr -2 & 1 } \ , \ D = \pmatrix{ 1 & 0 \cr 0 & 2 } \ , \ X^{-1} = \pmatrix{ -1 & -2 \cr -2 & -3 } $$

> Calc $$ X D X^{-1} = \pmatrix{ 3 & -2 \cr -2 & 1 } \ \pmatrix{ 1 & 0 \cr 0 & 2 } \ \pmatrix{ -1 & -2 \cr -2 & -3 } $$

>  $$ = \pmatrix{ 5 & 6 \cr -2 & -2 } = A $$


##Matrix Exponential

Power Series

$$ e^a = 1 + a + \cfrac{a^2}{2!} + \cfrac{a^3}{3!} + \dots + \cfrac{a^n}{n!} + \dots$$

$$ e^A  = I + A + \cfrac{1}{2!}A^2 + \cfrac{1}{3!}A^3 + \dots + \cfrac{1}{n!}A^n + \dots$$

If A is diagonal

$$ D = \pmatrix{ \lambda_1 & 0 & 0 & 0 \\ 0 & \lambda_2 & 0 & 0 \\ 0 & 0 & \ddots & 0 \\ 0 & 0 & 0 & \lambda_n} $$

$$ e^D = \lim_{m \rightarrow \infty} (I + D + \cfrac{1}{2!}D^2 + \cfrac{1}{3!}D^3 + \dots + \cfrac{1}{m!}D^m $$

$$ = \lim_{m \rightarrow \infty} \pmatrix{ \sum_{k=0}^{m} \frac{1}{k!}\lambda_1^k & 0 & 0 \cr 0 & \ddots & 0 \cr 0 & 0 &  \sum_{k=0}^{m} \frac{1}{k!}\lambda_n^k} $$

$$ = \pmatrix{e^{\lambda_1} & 0 & 0 & 0 \\ 0 & e^{\lambda_2} & 0 & 0 \\ 0 & 0 & \ddots & 0 \\ 0 & 0 & 0 & e^{\lambda_n}} $$

If $$ A $$ is diagonal,

$$ A^k = X D^k X^{-1} $$

$$ e^A = X(I + D + \cfrac{1}{2!}D^2 + \dots) X^{-1} = X e^D X^{-1}$$

Compute $$ e^A $$ for $$ A = \pmatrix{ 2 & 3 \cr -1 & -2 } $$

then Diagonal 

>   $$\matrix{ A & \lambda_1 = 1 & x_1 = \pmatrix{ 3 \\ -1} \\ \space & \lambda_2 = -1 & x_2 = \pmatrix{ 1 \\ -1 }} $$

>   $$ X = \pmatrix{3 & 1 \\ -1 & -1} \ , \ e^D  = \pmatrix{e^1 & 0 \\ 0 & e^-1} $$

>   $$ e^A = Xe^D X^{-1} =  \pmatrix{3 & 1 \\ -1 & -1} \ \pmatrix{e^1 & 0 \\ 0 & e^-1} \pmatrix{ \frac{1}{2} & \frac{1}{2} \\ -\frac{1}{2} & -\frac{3}{2}}$$

>   $$ e^A = \cfrac{1}{2}\pmatrix{ 3e-e^{-1} & 3e-3e^{-1} \cr -e+e^{-1} & -e+3e^{-1} } $$

### 6.2

$$ Y^\prime = AY \ , \ \cfrac{d}{dt}e^{at} = ae^{at} $$

$$ \cfrac{d}{dt}e^{tA} = Ae^{tA} $$

Let $$ Y(t) = e^{tA}Y(0)  $$ for $$ Y_0 = Y(0) $$

$$ Y^\prime(t) = Ae^{tA}Y_0 = AY $$

If A is Diagonal, 

$$ e^{tA} = Xe^{tD}X_1 \implies Y(t) = xe^{tD}X^{-1}Y_0 = Xe^{tD}c \space for \space c=x^{-1}Y_0 $$

$$ e^{tD} = \pmatrix{e^{t \lambda_1} & 0 & 0 \\ 0 & \ddots & 0 \\ 0 & 0 & e^{ t \lambda_n}} $$

$$ Y(t) = c_1 x_1 e^{\lambda_1 t} + c_2 x_2 e^{\lambda_2 t} + \dots + c_n x_n e^{\lambda_n t} $$

# Class Notes 20140409
## 6.1

1a) $$ A = \pmatrix{ 3 & 2 \cr 4 & 1 } \space \bigg\vert A-\lambda I \bigg\vert = \pmatrix{ (3-\lambda) & 2 \cr 4 & (1-\lambda) } $$

Characteristic Polynomial $$ \lambda^2 -4\lambda-5 = 0 = (3-\lambda) ( 1-\lambda) - 8 = 0$$

for $$ 2 \times 2 \space \ \space \lambda^2 - Tr(A)\lambda + \det(A) = 0 $$

$$ \lambda_1 = -1 \space \pmatrix{ 4 & 2 \cr 4 &  2 }x_1 = 0 \implies x_1 = \pmatrix{ 1 \\ -2 } $$

$$ \lambda_2 = 5 \space \pmatrix{ -2 & 2 \cr 5 & -5 }x_2 = 0 \implies x_2 = \pmatrix{ 1 \\ 1 } $$

> for
>>   $$ \lambda_1 $$ eigenspace $$ \alpha \pmatrix{1 \\ -2} $$ 

>>   $$ \lambda_2 $$ eigenspace $$ \alpha \pmatrix{1 \\ 1 } $$

1c)
>   $$ \pmatrix{ 3 & -1 \cr 1 & 1 } $$ Characteristic polynomial is $$ \lambda^2 -4\lambda +4 = 0 $$

>   $$ (\lambda - 2 )=0 \space \lambda_1 = \lambda_2 = 2 $$ Multiplicity of 2

>   $$ \pmatrix{ 1 & -1 \\ 1 & -1 } x_1 = 0 \implies x_1 = \pmatrix{ 1 \\ 1 } $$ Called Defective. 

1)a

>   $$ \pmatrix{ 1 & 1 & 1 \\ 0 & 2 & 1 \\ 0 & 0 & 1 } $$ since upper triangular $$ \implies  $$ Eigenvalues are diagonal elements. 

>   $$ \lambda_1 = \lambda_2 = 1 \space \lambda_3 = 2 $$

>   $$ \lambda_1 = \lambda_2 = 1 \space \pmatrix{ 0 & 1 & 1 \\ 0 & 1 & 1 \\ 0 & 0 & 0 }x = \pmatrix{ 0 \\ 0 \\ 0 } $$ 

>   $$ x = \pmatrix{ \alpha \\ \beta \\ -\beta } = \alpha\pmatrix{ 1 \\ 0 \\ 0} + \beta\pmatrix{ 0 \\ 1 \\ -1 } $$ two linearly independent Eigenvectors

>   $$ \lambda_3 = 2 \space \pmatrix{ -1 & 1 & 1 \\ 0 & 0 & 1 \\ 0 & ) & -1 }x_3 = 0, $$

>   $$ x_3 = \pmatrix{ a \\ b \\ c } \implies c=0 \implies -a+b=0 \implies a = b $$

>   $$ x_3 = \pmatrix{ 1 \\ 1 \\ 0 } $$

1i)
>   $$ \pmatrix{ 4 & -5 & 1 \\ 1 & 0 & -1 \\ 0 & 1 & -1 } $$ Characteristic Polynomial $$ \lambda(\lambda-1)(\lambda-2)=0 $$

>   $$ \lambda_1 = 0, \ \lambda_2 = 1, \ \lambda_3 = 2 $$

>   $$ \lambda_1 = 0 \space \pmatrix{ 4 & -5 & 1 \\ 1 & 0 & -1 \\ 0 & 1 & -1 }x_1 = 0 \implies x_1 = \pmatrix{ 1 \\ 1 \\ 1 } $$

>   $$ \lambda_2 = 0 \space \pmatrix{ 3 & -5 & 1 \\ 1 & -1 & -1 \\ 0 & 1 & -2 }x_2 = 1 \implies x_2 = \pmatrix{ 3 \\ 2 \\ 1 } $$

>   $$ \lambda_3 = 0 \space \pmatrix{ 2 & -5 & 1 \\ 1 & -2 & -1 \\ 0 & 1 & -3 }x_3 = 2 \implies x_3 = \pmatrix{ 7 \\ 3 \\ 1 } $$

1l)

>   $$ \lambda_1 = 1, \ x_1 = \pmatrix{ 0 \\ 1 \\ 0 \\ 0 } $$

>   $$ \lambda_2 = \lambda_3 = 2, \ x_1 = \pmatrix{ 0 \\ 0 \\ 1 \\ 0 } $$

>   $$ \lambda_4 = 3, \ x_3 = \pmatrix{ 1 \\ 2 \\ 0 \\ 0 } $$

> Defective since $$ \lambda_2 = \lambda_3 = 2 $$ only has 1 eigenvector. 

$$ A $$ is $$ n \times n \det( A - 0 \cdot I ) = 0 \implies \lambda = 0 $$ is eigenvalue.

If $$ \lambda = 0 $$ is an eigenvalue $$ \implies \det( A - 0 \cdot I ) = 0 \implies \det ( A ) = 0 \implies A $$ is singular

6)

> $$ \lambda $$ is eigenvalue for $$ A $$ with corresponding egienvector $$ x $$ Show $$ \lambda^m $$ is an eigenvalue od $$ A^m $$ with eigenvector $$ x $$

>  $$ m=1 \space Ax = \lambda x $$

>  $$ m=2 \space A^2 x = \lambda A x = \lambda \lambda x \implies A^2 x = \lambda ^2 x \implies \lambda^2 $$ is an eigenvalue of $$ A^2 $$ with corresponding eigenvector $$ x $$

>  Assume true for $$ m=k \space A^k x = \lambda^k x$$ Show true for $$ m= k+1 $$

>  $$ A( A^k x ) = A( \lambda^k x ) $$

>  $$ A^{k+1} x = \lambda^k A x = \lambda^k \lambda x = \lambda^{k+1} x$$

9)

> An $$ n \times n $$ matrix $$ A $$ is nilpotent if $$ A^k = 0 $$ for some positive integer k. Show all eigenvalues are 0.

>  If $$ A^k = 0 \implies  $$ all eigenvectors of $$ A^k $$ are 0. But eigenvalues of $$ A^k $$ are $$ \lambda^k \implies \lambda^k = 0 \implies \lambda = 0 $$

## 6.2

1a)

>  $$ y_1^\prime = y_1 + y_2 \ , \ Y=\pmatrix{y_1 \\ y_2}$$

>  $$ y_2^\prime = -2y_1 + 4y_2 $$

>  $$ Y^\prime \pmatrix{ 1 & 1 \\ -2 & 4 }Y = AY $$

>  $$ A = \pmatrix{ 1 & 1 \cr -2 & 4 } \ , \ \lambda^2 - 5\lambda + 6 = 0 \implies (\lambda - 3 )( \lambda - 2) = 0$$

>  $$ \lambda_1 = 2 \ \pmatrix{ -1 & 1 \\ -2 & 2 } x_1 = 0 \implies x_1 = \pmatrix{ 1 \\ 1 } $$

>  $$ \lambda_2 = 3 \ \pmatrix{ -2 & 1 \\ -2 & 1 } x_2 = 0 \implies x_2 = \pmatrix{ 1 \\ 2 } $$

>  $$ Y = c_1 \pmatrix{ 1 \\ 1 }e^{2t} + c_2 \pmatrix{ 1 \\ 2 }e^{3t} $$

>  $$ y_1 = c_1e^{2t} + c_2 e^{3t}  \ , \ y_2 = c_1 e^{2t} + 2c_2 e^{3t} $$

1c)

>  $$ Y' = AY \ , \ A = \pmatrix{ 1 & -2 \cr -2 & 4 } \lambda^2 - 5\lambda = 0 \ , \ \lambda(\lambda - 5) = 0 $$

>  $$ \lambda_1 = 0 \ , \ \pmatrix{ 1 & -2 \cr -2 & 4 }x_1 = 0 \implies x_1 = \pmatrix{ 2 \\ 1 } $$

>  $$ \lambda_2 = 5 \ , \ \pmatrix{ -4 & -2 \cr -2 & -1 }x_2 = 0 \implies x_2 = \pmatrix{ 1 \\ -2 } $$

> then $$ Y = c_1 \pmatrix{ 2 \\ 1 } e^{0 \cdot t} + c_2 \pmatrix{ 1 \\ -2 } e^{5t}$$

>  $$ y_1 = 2c_1 + c_2 e^{5t} $$

>  $$ y_2 = c_1 + 2c_2 e^{5t} $$

1f)

>  $$ A = \pmatrix{ 1 & 0 & 1 \\ 0 & 2 & 6 \\ 0 & 1 & 3 } \ , \ Y = \pmatrix{ y_1 \\ y_2 \\ y_3 } $$

>  $$ Y' = AY \bigg\vert A - \lambda I \bigg\vert = ( 1-\lambda )( \lambda^2 - 5\lambda) = ( 1 - \lambda )\lambda( \lambda - 5 ) = 0 $$

>  $$ \lambda_1 = 0 \ , \  \lambda_2 = 1 \ , \ \lambda_3 = 5 $$

>  $$ \lambda_1 = 0 \ , \ x_1 = \pmatrix{ -1 \\ -3 \\ 1 } $$

>  $$ \lambda_2 = 1 \ , \ x_2 = \pmatrix{ 1 \\ 0 \\ 0 } $$

>  $$ \lambda_3 = 5 \ , \ x_3 = \pmatrix{ 1 \\ 8 \\ 4 } $$

>  $$ Y = c_1 \pmatrix{-1 \\ -3 \\ 1} e^{0t} + c_2 \pmatrix{ 1 \\ 0 \\ 0 } e^t + c_3 \pmatrix{ 1 \\ 8 \\ 4 } e^{5t} $$

>  $$ y_1 = -c_1 + c_2 e^t + c_3 e^{5t} $$

>  $$ y_2 = -3c_1 + 8c_3e^{5t} $$

>  $$ y_3 = c_1 + 4c_3 e^{5t} $$

2a)

>  $$ A = \pmatrix{-1 & 2 \\ 2 & -1} \ , \ \lambda^2 + 2\lambda -3 = 0 \implies (\lambda + 3)(\lambda - 1) = 0 $$

>  $$ \lambda_1 = -3 \ , \ x_1 = \pmatrix{ 1  \\ -1  } $$

>  $$ \lambda_2 = 1 \ , \ x_2 = \pmatrix{ 1 \\ 1 } $$

>  $$ Y(0) = \pmatrix{ 3 \\ 1 } $$

>  $$ Y = c_1 \pmatrix{ 1 \\ -1 } e^{-3t} + c_2 \pmatrix{ 1 \\ 1 }e^t $$

>  $$ Y(0) = c_1 \pmatrix{ 1 \\ -1 } + c_2 \pmatrix{ 1 \\ 1 } = \pmatrix{ 1 \\ 3 } \implies c_1 = 1 \ , \ c_2 = 2 $$

>  from $$ \pmatrix{ 1 & 1 \cr -1 & 1 } \pmatrix{ c_1 \\ c_2 } = \pmatrix{ 3 \\ 1 } $$

Given $$  Y = c_1 x_1 e^{\lambda_1 t } + c_2 x_2 e^{\lambda_2 t} + \dots + c_n x_n e^{\lambda_n t} $$ is a solution to
   $$ Y' = AY \ , \ Y(0) = Y_0 $$

a) 
>  $$ t = 0 \implies c_1 x_1 + c_2 x_2 + \dots + c_n x_n = Y_0 \implies \pmatrix{ x_1 & x_2 & \cdots & x_n } \pmatrix{ c_1 \\ c_2 \\ \vdots \\ c_n } = Y_0 $$

>  $$ x = \pmatrix{x_1 & x_2 \dots x_n } \ , \ \ Xc = Y_0 $$

>  $$ c = \pmatrix{ c_1 \\ \vdots \\ c_2 } \ , \ x_i $$'s Linear independent $$ \implies \det(x) = 0 $$

>  $$ \implies X^{-1}$$ exists $$ \implies X^{-1}(Xc)= X^{-1}Y_0 $$ or $$ c = X^{-1}Y_0 $$

4)
>  Let $$ S_1 \equiv $$ amount of salt in tank A and $$ S_2 \equiv $$ amount of salt in tank B

>  $$ \cfrac{ds_1}{dt} = -16\pmatrix{\cfrac{s_1}{100}} + 4 \pmatrix{\cfrac{s_2}{100}} $$

>  $$ \cfrac{ds_2}{dt} = 16\pmatrix{\cfrac{s_1}{100}} - 4 |pmatrix \cfrac{s_2}{100} - 12 \pmatrix{ \cfrac{s_3}{100}} $$

>  $$ \implies S' = AS $$ where $$ S = \pmatrix{ s_1 \\ s_2 } $$

>  $$ A = \pmatrix{ -0.16 & +0.04 \\ 0.16 & -0.16 } $$

>  $$ \lambda^2 + 0.32\lambda + 0.0192 = 0 \implies \lambda = -0.08 \ , \ \lambda = -0.24 $$

>  $$ \matrix{ s_1(0) = 40 \\ s_2(0) = 20} \bigg\rbrace \implies S(0) = \pmatrix{ 40 \\ 20 } $$

>  $$ \lambda_1 = -0.24 \ , \ x_1 = \pmatrix{ 1 \\ -2 } $$

>  $$ \lambda_2 = -0.08 \ , \ x_2 = \pmatrix{ 1 \\ 2 } $$

>  $$ S(t) = c_1 \pmatrix{1 \\ -2} e^{-0.24 t} + c_2 \pmatrix{ 1 \\ 2 } e ^{-0.008 t} $$

>  $$ S(0) \implies \ , \ c_1 = 15 \ , \ c_2 = 25 $$

>  $$ s_1(t) = 15 e^{-0.24 t} + 25 e^{-0/08 t} $$

>  $$ s_2(t) = -30 e^{-0.24 t} + 50 e^{-0.08 t} $$

>  $$ \lim_{t \rightarrow \infty } s_1(t) = \lim_{t \rightarrow\infty} s_2(t) = 0 $$

6)

>   $$ y_1'' = -2y_2 + y_1' + 2y_2' $$

>   $$ y_2'' = 2y_1 +2y - y_2' $$

>   Let $$ y_3 = y_1' \ , \ y_4 = y_2' \ , \  $$ Let $$ Y = \pmatrix{ y_1 \\ y_2 \\ y_3 \\ y_4 } \implies Y' = \pmatrix{ 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & -2 & 1 & 2 \\ 2 & 0 & 2 & -1 }Y$$

>   $$ Y = c_1 \pmatrix{ 1 \\ 2 \\ 1 \\ 2 } e^{t} + c_2 \pmatrix{ -2 \\ -1 \\ 2 \\ 1 } e^{-t} + c_3 \pmatrix{ 1 \\ 1 \\ 2 \\ 2 } e^{2t} + c_4 \pmatrix{ 1 \\ -1 \\ -2 \\ 2 } e^{-2t}$$

>   $$ Y(0) = \pmatrix{ 1 \\ 0 \\ -3 \\ 2 } \implies c_1 = 1 \ , \ c_2 = 0 \ , \ c_3 = -1 \ , \ c_4 = 1 $$

>   $$ y_1 = e^t - e^2t + e-2t $$

>   $$ y_2 = 2e^t - e^{2t} - e^{-2t} $$

## 6.4  Hermitian Matrices. 

Consider complex numbers $$ a = b i $$ Let $$ C^n $$ be n-tuples of complex numbers $$ v \in C^3 $$

$$ v = \pmatrix{ a_1 + b_1 i \\ a_2 + b_2 i \\ a_3 + b_3 i } $$

For $$  1 - D $$ if $$ \alpha = a + b i $$ then $$ \vert \alpha \vert = \sqrt{ \bar{\alpha} \alpha } = \sqrt{a^2 + b^2} $$

$$  \alpha = a + b \bar{i} \qquad \bar{\alpha} = a - b i $$

For $$ z \in C^n $$

$$ \bigg\Vert z \bigg\Vert = \pmatrix{ \bigg\vert z_1 \bigg\vert ^2 + \bigg\vert z_2 \bigg\vert ^ 2 + \dots + \bigg\vert z_n \bigg\vert ^2} ^{\frac{1}{2}} $$ 

or 

$$ \bigg\Vert z \bigg\Vert = \pmatrix{\overline{Z}^T Z}^{\frac{1}{2}} $$

Define 

$$ Z^H = \overline{Z}^T $$

## Inner product space on $$ V $$. 

####Def:
1.   $$ \langle z, z, \rangle \ge 0 \qquad  ( = 0 \iff z = 0 )$$

2.   $$ \langle z , w \rangle = \overline{\langle w, u \rangle} \quad \forall \space z \ , \ w  \ \in \ V $$

3.   $$ \langle \alpha z + \beta w \ , \ u \rangle = \alpha \langle z \ , \ u \rangle + \beta \langle w \ , \ u \rangle$$

For an Orthoganal basis, $$ \bigg\lbrace w_1 , w_2, \dots , w_n \bigg\rbrace $$ and if 

$$ z = \sum_{i=1}^{n} c_i w_i $$

then 

$$ c_i = \langle z \ , \ w_i \rangle $$

$$ \overline{c_i} = \langle w_i \ , \ z \rangle $$

and 

$$ \bigg\Vert z \bigg\Vert^2 = \sum_{i=1}^{n}c_i \overline{c_i} $$

We will define $$ \langle z \ , \ w \rangle = W^HZ $$ 

>  If $$ Z = \pmatrix{ 1 + 2i \\ 4 - 3i} \qquad w = \pmatrix{4-i \\ 2+i} $$

>   $$ \langle z \ , \ w \rangle = W^HZ = \pmatrix{ 4+i & 2-i } \pmatrix{1+2i \\ 4-3i} = 7-4i$$

>   $$ Z^HZ = \bigg\vert 1 + 2i \bigg\vert ^2 + \bigg\vert 4 - 3i \bigg\vert^2 = (1 + 4) + (16 + 9) = 30$$

>   $$ \bigg\Vert Z \bigg\Vert = \sqrt{Z^HZ} = \sqrt{30} $$

### Hermitian Matrices

Let $$ M = (a_{ij} + i b_{ij}) \qquad i=\sqrt{-1} $$ Assume $$ a_{ij} $$ and $$ b_{ij} $$ are real. 

$$ M = A+Bi $$

$$ \overline{M} = A-Bi $$

$$ M^H = \overline{M}^T $$

$$ A^HA = \overline{A}^TA = A^TA $$

$$ \pmatrix{A^H}^H = A $$

and 

$$ \pmatrix{\alpha A + \beta B}^H = \overline{\alpha}A^H + \overline{\beta}B^H = \overline{\alpha}A^T + \overline{\beta}B^T $$

$$ \pmatrix{AC}^H = \pmatrix{\overline{AC}}^T = \pmatrix{ \overline{A} & \overline{C}}^T = \overline{C}^T \ \overline{A}^T = C^H \ A^H$$

#### def:-

A matrix is called Hermitian if $$ M^H = M $$

#### e.g.

>   $$ M = \pmatrix{ 4 & 3+2i \cr 3-2i & 3 } $$

>   $$ M^H = \overline{M}^T = \pmatrix{ 4 & 3-2i \cr 3+2i & 3 }^T = \pmatrix{ 4 & 3+2i \cr 3-2i & 3 }$$

The Diagonals are real. Matching offset diagonals are conjugates

If $$ M $$ is real $$ M^H = M^T $$

If M is Hermitian $$ \implies M^T = M \implies M $$ is symetric


#### Thm:-

All eigenvalues of a Hermitian matrix are real. Also eigenvectors belonging to distinct eigenvalues are orthogonal

#### e.g.

>   $$ A = \pmatrix{ 2 & 1+i \cr 1-i & 3 } $$

>   $$ \bigg\vert A - \lambda I \bigg\vert = \bigg\vert\matrix{ 2-\lambda & 1+i \cr 1-i & 3-\lambda }\bigg\vert = (2-\lambda)(3-\lambda)-(1+i)(1-i) = 0 $$

>   $$ \implies \lambda^2 -5\lambda + 6 -2 = 0$$

>   $$ \lambda^2 -5\lambda +4 = 0 $$

>   $$ \lambda_1 = 1 \qquad \lambda_2 = 4 $$

>   $$ \lambda_1 = 1 \qquad \pmatrix{ 1 & 1+i \cr 1-i & 2 }x_1 = 0 \implies x_1 = \pmatrix{-1-i \\ 1} $$ or $$ x_1 = \pmatrix{1+i \\ -1} $$

>   $$ \lambda_2 = 4 \qquad \pmatrix{ -2 & 1+i \cr 1-i & -1 }x_2 = 0 \qquad x_2 = \pmatrix{ 1+i \\ 2} $$

>   $$ \langle x_2 \ , \ \rangle = x_1^Hx_2 = \pmatrix{ 1-i & -1 } \pmatrix{ 1_i \\ 2 } = 2-2 = 0 \implies x_1 \perp x_2$$ orthogonal

#### def:-
$$ U \space n \times n $$ is called unitary if its collumn vectors form an orthogonal set in $$ C^n $$

if $$ U $$ is unoitary $$ U^HU = I \qquad U^{-1}= U^H$$

#### Thm:-

If the eigenvalues of a Hermitian matrix $$ A $$ are distinct, then there exists a unitary matrix that diagonalizes $$ A $$

#### e.g.

>   $$ A = \pmatrix{ 1 & 1+i \cr 1-i & 3 } $$ then let $$ u_1 = \cfrac{x_1}{\Vert x_1 \Vert} \qquad u_2 = \cfrac{x_2}{\Vert x_2 \Vert} $$

>   $$ U = \pmatrix{ \frac{-1-i}{\sqrt{3}} & \frac{1+i}{\sqrt{6}} \cr \frac{1}{\sqrt{3}} & \frac{2}{\sqrt{6}} } = \cfrac{1}{\sqrt{3}}\pmatrix{ -1-i & \frac{1+i}{\sqrt{2}} \cr 1 & \sqrt{2} } $$

>   $$ U^H = \cfrac{1}{\sqrt{3}} \pmatrix{ -1+i & 1 \cr \frac{1-i}{\sqrt{2}} & \sqrt{2} } $$

Show $$ U^HAU = \pmatrix{ 1 & 0 \cr 0 & 4 } $$

### Normal Matrices

#### def:-
A Matrix $$ A $$ is normal if $$ AA^H = A^HA $$

#### thm:- 
A matrix $$ A $$ is normal $$ \iff $$ it contains a complete orthonormal set of Eigenvalues. 

>   $$ A = \pmatrix{ 2 & i & 0 \\ -i & 2 & 0 \\ 0 & 0 & 2 } \qquad A^H = A $$ 

>   $$ \implies AA^H = A^2 = A^HA $$

#### e.g.

>   $$ A = \pmatrix{ 0 & 0 & 1 \\ 0 & 1 & 0 \\ 1 & 0 & 0 } \qquad A^H = A $$

>   The Characteristic polynimial $$ \implies ( 1 - \lambda )( \lambda^2 - 1 )=0 \implies \lambda_1 = -1 \quad \lambda_2 = \lambda_3 = 1 $$

>   $$ \lambda = -1 \quad x_1 = \pmatrix{ -1 \\ 0 \\ 1} $$

>   $$ \lambda_2 = \lambda_3 = 1 \quad x_2 = \pmatrix{ 0 \\ 1 \\ 0 } \quad x_3 = \pmatrix{ 1 \\ 0 \\ 1 } $$

>   $$ U_1 = \cfrac{x_1}{\Vert x_1 \Vert} = \cfrac{1}{\sqrt{2}}\pmatrix{ -1 \\ 0 \\ 1 } \quad u_2 = \pmatrix{ 0 \\ 1 \\ 0 } \quad u_3 = \cfrac{1}{\sqrt{2}} \pmatrix{ 1 \\ 0 \\ 1} $$

>   $$ U = \cfrac{1}{\sqrt{2}} \pmatrix{ -1 & 0 & 1 \\ 0 & \sqrt{2} & 0 \\ 1 & 0 & 1 } \space $$ is Orthogonal